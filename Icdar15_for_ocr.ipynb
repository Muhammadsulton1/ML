{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1g3MdbeZt19FHG7z7EjIhf-0lvk_6XZeF",
      "authorship_tag": "ABX9TyN0sOfImVeEpe5d5PY07Jv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammadsulton1/ML/blob/main/Icdar15_for_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms  \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "WBLeXEKdieDE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def statistics_label_cnt(lbl_path, lbl_cnt_map):\n",
        "    \"\"\"\n",
        "    统计标签文件中都包含哪些label以及各自出现的次数\n",
        "    \"\"\"\n",
        "    with open(lbl_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split(',')\n",
        "            img_name = items[0]\n",
        "            lbl_str = items[1].strip()[1:-1]\n",
        "            for lbl in lbl_str:\n",
        "                if lbl not in lbl_cnt_map.keys():\n",
        "                    lbl_cnt_map[lbl] = 1\n",
        "                else:\n",
        "                    lbl_cnt_map[lbl] += 1\n",
        "\n",
        "\n",
        "def statistics_max_len_label(lbl_path):\n",
        "    \"\"\"\n",
        "    统计标签文件中最长的label所包含的字符数\n",
        "    \"\"\"\n",
        "    max_len = -1\n",
        "    with open(lbl_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split(',')\n",
        "            img_name = items[0]\n",
        "            lbl_str = items[1].strip()[1:-1]\n",
        "            lbl_len = len(lbl_str)\n",
        "            max_len = max_len if max_len > lbl_len else lbl_len\n",
        "    return max_len\n",
        "\n",
        "\n",
        "def load_lbl2id_map(lbl2id_map_path):\n",
        "    \"\"\"\n",
        "    读取label-id映射关系记录文件\n",
        "    \"\"\"\n",
        "    lbl2id_map = dict()\n",
        "    id2lbl_map = dict()\n",
        "    with open(lbl2id_map_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split('\\t')\n",
        "            label = items[0]\n",
        "            cur_id = int(items[1])\n",
        "            lbl2id_map[label] = cur_id\n",
        "            id2lbl_map[cur_id] = label\n",
        "    return lbl2id_map, id2lbl_map"
      ],
      "metadata": {
        "id": "F8LxTSMOieFk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_data_dir = '/content/drive/MyDrive/icdar15'\n",
        "\n",
        "train_img_dir = os.path.join(base_data_dir, 'train')\n",
        "valid_img_dir = os.path.join(base_data_dir, 'test')\n",
        "train_lbl_path = os.path.join(base_data_dir, 'gt.txt')\n",
        "valid_lbl_path = os.path.join(base_data_dir, 'test_gt.txt')\n",
        "lbl2id_map_path = os.path.join(base_data_dir, 'lbl2id_map.txt')\n",
        "\n",
        "# 统计数据集中出现的所有的label中包含字符最多的有多少字符\n",
        "train_max_label_len = statistics_max_len_label(train_lbl_path)\n",
        "valid_max_label_len = statistics_max_len_label(valid_lbl_path)\n",
        "max_label_len = max(train_max_label_len, valid_max_label_len)\n",
        "print(f\"数据集中包含字符最多的label长度为{max_label_len}\")\n",
        "\n",
        "# 统计数据集中出现的所有的符号\n",
        "lbl_cnt_map = dict()\n",
        "statistics_label_cnt(train_lbl_path, lbl_cnt_map)\n",
        "print(\"训练集中出现的label\")\n",
        "print(lbl_cnt_map)\n",
        "statistics_label_cnt(valid_lbl_path, lbl_cnt_map)\n",
        "print(\"训练集+验证集中出现的label\")\n",
        "print(lbl_cnt_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp_aQdJvieIO",
        "outputId": "f2b7d57e-ac08-458e-db68-7db214791f33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据集中包含字符最多的label长度为21\n",
            "训练集中出现的label\n",
            "{'G': 352, 'e': 1080, 'n': 623, 'a': 858, 'x': 28, 'i': 666, 's': 577, ' ': 54, 'T': 918, 'h': 308, 't': 582, 'r': 664, '[': 2, '0': 185, '6': 39, ']': 2, '2': 120, '-': 69, '3': 50, 'C': 606, 'p': 203, 'k': 100, 'E': 1474, 'X': 113, 'I': 879, 'R': 861, 'f': 141, 'u': 298, 'o': 685, 'l': 427, 'v': 123, 'A': 1229, 'U': 333, 'O': 1007, 'N': 805, 'c': 331, 'm': 216, 'W': 185, 'H': 408, 'Y': 237, 'P': 406, 'F': 269, '?': 5, 'S': 1199, 'b': 92, 'g': 173, 'L': 768, 'M': 375, 'D': 394, 'd': 259, '$': 47, '5': 76, '4': 44, '.': 96, 'w': 99, 'B': 336, '1': 184, '7': 44, '8': 44, 'V': 161, 'y': 165, 'K': 171, '!': 53, '9': 66, 'z': 12, ';': 3, '#': 16, 'j': 15, \"'\": 51, 'J': 77, ':': 19, '%': 28, '/': 24, 'q': 3, 'Q': 19, '(': 6, ')': 5, '\\\\': 8, '\"': 8, '´': 3, 'Z': 29, '&': 9, 'É': 1, '@': 4, '=': 1, '+': 1}\n",
            "训练集+验证集中出现的label\n",
            "{'G': 536, 'e': 1575, 'n': 910, 'a': 1230, 'x': 39, 'i': 948, 's': 816, ' ': 86, 'T': 1353, 'h': 431, 't': 852, 'r': 957, '[': 2, '0': 235, '6': 45, ']': 2, '2': 140, '-': 88, '3': 71, 'C': 919, 'p': 323, 'k': 143, 'E': 2314, 'X': 189, 'I': 1269, 'R': 1307, 'f': 211, 'u': 423, 'o': 982, 'l': 587, 'v': 171, 'A': 1888, 'U': 488, 'O': 1518, 'N': 1190, 'c': 458, 'm': 292, 'W': 297, 'H': 616, 'Y': 355, 'P': 603, 'F': 419, '?': 7, 'S': 1806, 'b': 135, 'g': 265, 'L': 1163, 'M': 551, 'D': 569, 'd': 374, '$': 58, '5': 99, '4': 53, '.': 135, 'w': 138, 'B': 481, '1': 229, '7': 61, '8': 51, 'V': 234, 'y': 237, 'K': 267, '!': 68, '9': 76, 'z': 14, ';': 3, '#': 24, 'j': 19, \"'\": 71, 'J': 107, ':': 24, '%': 42, '/': 29, 'q': 3, 'Q': 28, '(': 7, ')': 5, '\\\\': 8, '\"': 8, '´': 3, 'Z': 36, '&': 15, 'É': 1, '@': 9, '=': 1, '+': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_max_label_len = statistics_max_len_label(train_lbl_path)\n",
        "valid_max_label_len = statistics_max_len_label(valid_lbl_path)\n",
        "max_label_len = max(train_max_label_len, valid_max_label_len)\n",
        "print(f\"数据集中包含字符最多的label长度为{max_label_len}\")\n",
        "\n",
        "# 统计数据集中出现的所有的符号\n",
        "lbl_cnt_map = dict()\n",
        "statistics_label_cnt(train_lbl_path, lbl_cnt_map)\n",
        "print(\"训练集中出现的label\")\n",
        "print(lbl_cnt_map)\n",
        "statistics_label_cnt(valid_lbl_path, lbl_cnt_map)\n",
        "print(\"训练集+验证集中出现的label\")\n",
        "print(lbl_cnt_map)\n",
        "\n",
        "# 构造 label - id 之间的映射\n",
        "print(\"\\n\\n构造 label - id 之间的映射\")\n",
        "lbl2id_map = dict()\n",
        "# 初始化两个特殊字符\n",
        "lbl2id_map['☯'] = 0    # padding标识符\n",
        "lbl2id_map['■'] = 1    # 句子起始符\n",
        "lbl2id_map['□'] = 2    # 句子结束符\n",
        "# 生成其余label的id映射关系\n",
        "cur_id = 3\n",
        "for lbl in lbl_cnt_map.keys():\n",
        "    lbl2id_map[lbl] = cur_id\n",
        "    cur_id += 1\n",
        "# 保存 label - id 之间的映射\n",
        "with open(lbl2id_map_path, 'w') as writer:\n",
        "    for lbl in lbl2id_map.keys():\n",
        "        cur_id = lbl2id_map[lbl]\n",
        "        print(lbl, cur_id)\n",
        "        line = lbl + '\\t' + str(cur_id) + '\\n'\n",
        "        writer.write(line)\n",
        "\n",
        "# 分析数据集图片尺寸"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbSHGRNTieKs",
        "outputId": "87a8a68c-f484-4937-ea37-996e25932010"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据集中包含字符最多的label长度为21\n",
            "训练集中出现的label\n",
            "{'G': 352, 'e': 1080, 'n': 623, 'a': 858, 'x': 28, 'i': 666, 's': 577, ' ': 54, 'T': 918, 'h': 308, 't': 582, 'r': 664, '[': 2, '0': 185, '6': 39, ']': 2, '2': 120, '-': 69, '3': 50, 'C': 606, 'p': 203, 'k': 100, 'E': 1474, 'X': 113, 'I': 879, 'R': 861, 'f': 141, 'u': 298, 'o': 685, 'l': 427, 'v': 123, 'A': 1229, 'U': 333, 'O': 1007, 'N': 805, 'c': 331, 'm': 216, 'W': 185, 'H': 408, 'Y': 237, 'P': 406, 'F': 269, '?': 5, 'S': 1199, 'b': 92, 'g': 173, 'L': 768, 'M': 375, 'D': 394, 'd': 259, '$': 47, '5': 76, '4': 44, '.': 96, 'w': 99, 'B': 336, '1': 184, '7': 44, '8': 44, 'V': 161, 'y': 165, 'K': 171, '!': 53, '9': 66, 'z': 12, ';': 3, '#': 16, 'j': 15, \"'\": 51, 'J': 77, ':': 19, '%': 28, '/': 24, 'q': 3, 'Q': 19, '(': 6, ')': 5, '\\\\': 8, '\"': 8, '´': 3, 'Z': 29, '&': 9, 'É': 1, '@': 4, '=': 1, '+': 1}\n",
            "训练集+验证集中出现的label\n",
            "{'G': 536, 'e': 1575, 'n': 910, 'a': 1230, 'x': 39, 'i': 948, 's': 816, ' ': 86, 'T': 1353, 'h': 431, 't': 852, 'r': 957, '[': 2, '0': 235, '6': 45, ']': 2, '2': 140, '-': 88, '3': 71, 'C': 919, 'p': 323, 'k': 143, 'E': 2314, 'X': 189, 'I': 1269, 'R': 1307, 'f': 211, 'u': 423, 'o': 982, 'l': 587, 'v': 171, 'A': 1888, 'U': 488, 'O': 1518, 'N': 1190, 'c': 458, 'm': 292, 'W': 297, 'H': 616, 'Y': 355, 'P': 603, 'F': 419, '?': 7, 'S': 1806, 'b': 135, 'g': 265, 'L': 1163, 'M': 551, 'D': 569, 'd': 374, '$': 58, '5': 99, '4': 53, '.': 135, 'w': 138, 'B': 481, '1': 229, '7': 61, '8': 51, 'V': 234, 'y': 237, 'K': 267, '!': 68, '9': 76, 'z': 14, ';': 3, '#': 24, 'j': 19, \"'\": 71, 'J': 107, ':': 24, '%': 42, '/': 29, 'q': 3, 'Q': 28, '(': 7, ')': 5, '\\\\': 8, '\"': 8, '´': 3, 'Z': 36, '&': 15, 'É': 1, '@': 9, '=': 1, '+': 2}\n",
            "\n",
            "\n",
            "构造 label - id 之间的映射\n",
            "☯ 0\n",
            "■ 1\n",
            "□ 2\n",
            "G 3\n",
            "e 4\n",
            "n 5\n",
            "a 6\n",
            "x 7\n",
            "i 8\n",
            "s 9\n",
            "  10\n",
            "T 11\n",
            "h 12\n",
            "t 13\n",
            "r 14\n",
            "[ 15\n",
            "0 16\n",
            "6 17\n",
            "] 18\n",
            "2 19\n",
            "- 20\n",
            "3 21\n",
            "C 22\n",
            "p 23\n",
            "k 24\n",
            "E 25\n",
            "X 26\n",
            "I 27\n",
            "R 28\n",
            "f 29\n",
            "u 30\n",
            "o 31\n",
            "l 32\n",
            "v 33\n",
            "A 34\n",
            "U 35\n",
            "O 36\n",
            "N 37\n",
            "c 38\n",
            "m 39\n",
            "W 40\n",
            "H 41\n",
            "Y 42\n",
            "P 43\n",
            "F 44\n",
            "? 45\n",
            "S 46\n",
            "b 47\n",
            "g 48\n",
            "L 49\n",
            "M 50\n",
            "D 51\n",
            "d 52\n",
            "$ 53\n",
            "5 54\n",
            "4 55\n",
            ". 56\n",
            "w 57\n",
            "B 58\n",
            "1 59\n",
            "7 60\n",
            "8 61\n",
            "V 62\n",
            "y 63\n",
            "K 64\n",
            "! 65\n",
            "9 66\n",
            "z 67\n",
            "; 68\n",
            "# 69\n",
            "j 70\n",
            "' 71\n",
            "J 72\n",
            ": 73\n",
            "% 74\n",
            "/ 75\n",
            "q 76\n",
            "Q 77\n",
            "( 78\n",
            ") 79\n",
            "\\ 80\n",
            "\" 81\n",
            "´ 82\n",
            "Z 83\n",
            "& 84\n",
            "É 85\n",
            "@ 86\n",
            "= 87\n",
            "+ 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n 分析数据集图片尺寸\")\n",
        "min_h = 1e10\n",
        "min_w = 1e10\n",
        "max_h = -1\n",
        "max_w = -1\n",
        "min_ratio = 1e10\n",
        "max_ratio = 0\n",
        "for img_name in os.listdir(train_img_dir):\n",
        "    img_path = os.path.join(train_img_dir, img_name)\n",
        "    img = cv2.imread(img_path)\n",
        "    h, w = img.shape[:2]\n",
        "    ratio = w / h\n",
        "    min_h = min_h if min_h <= h else h\n",
        "    max_h = max_h if max_h >= h else h\n",
        "    min_w = min_w if min_w <= w else w \n",
        "    max_w = max_w if max_w >= w else w \n",
        "    min_ratio = min_ratio if min_ratio <= ratio else ratio\n",
        "    max_ratio = max_ratio if max_ratio >= ratio else ratio\n",
        "print(\"min_h\", min_h)\n",
        "print(\"max_h\", max_h)\n",
        "print(\"min_w\", min_w)\n",
        "print(\"max_w\", max_w)\n",
        "print(\"min_ratio\", min_ratio)\n",
        "print(\"max_ratio\", max_ratio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2o5y3GGieNE",
        "outputId": "b88c5a44-38e8-4953-fef0-848e5c404d5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " 分析数据集图片尺寸\n",
            "min_h 9\n",
            "max_h 658\n",
            "min_w 15\n",
            "max_w 628\n",
            "min_ratio 0.09302325581395349\n",
            "max_ratio 8.619047619047619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. \n",
        "    Base for this and many other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed    # input embedding module(input embedding + positional encode)\n",
        "        self.tgt_embed = tgt_embed    # ouput embedding module\n",
        "        self.generator = generator    # output generation module\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        memory = self.encode(src, src_mask)\n",
        "        res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
        "        return res\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        src_embedds = self.src_embed(src)\n",
        "        return self.encoder(src_embedds, src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        target_embedds = self.tgt_embed(tgt)\n",
        "        return self.decoder(target_embedds, memory, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder\n",
        "    The encoder is composed of a stack of N=6 identical layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# We employ a residual connection around each of the two sub-layers, followed by layer normalization\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, feature_size, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(feature_size))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(feature_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        sublayer_out = sublayer(x)\n",
        "        sublayer_out = self.dropout(sublayer_out)\n",
        "        x_norm = x + self.norm(sublayer_out)\n",
        "        return x_norm\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size   # embedding's dimention of model, 默认512\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        # attention sub layer\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        # feed forward sub layer\n",
        "        z = self.sublayer[1](x, self.feed_forward)\n",
        "        return z\n",
        "\n",
        "\n",
        "# Decoder\n",
        "# The decoder is also composed of a stack of N=6 identical layers.\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "\n",
        "# Attention\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "\n",
        "# Embeddings and Softmax\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedds = self.lut(x)\n",
        "        return embedds * math.sqrt(self.d_model)    # TODO 这里的归一化操作的目的?\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # TODO 位置信息编码的具体原理\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "# Full Model\n",
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"\"\"\n",
        "    构建模型\n",
        "    params:\n",
        "        src_vocab:\n",
        "        tgt_vocab:\n",
        "        N: 编码器和解码器堆叠基础模块的个数\n",
        "        d_model: 模型中embedding的size，默认512\n",
        "        d_ff: FeedForward Layer层中embedding的size，默认2048\n",
        "        h: MultiHeadAttention中多头的个数，必须被d_model整除\n",
        "        dropout:\n",
        "    \"\"\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "metadata": {
        "id": "gUbfgAE687hO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
        "\n",
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        \"\"\"\n",
        "        norm: loss的归一化系数，用batch中所有有效token数即可\n",
        "        \"\"\"\n",
        "        x = self.generator(x)\n",
        "        x_ = x.contiguous().view(-1, x.size(-1))\n",
        "        y_ = y.contiguous().view(-1)\n",
        "        loss = self.criterion(x_, y_)\n",
        "        loss /= norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        #return loss.data[0] * norm  # TODO\n",
        "        return loss.item() * norm"
      ],
      "metadata": {
        "id": "pXRndO3w9BA7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Recognition_Dataset(object):\n",
        "\n",
        "    def __init__(self, dataset_root_dir, lbl2id_map, sequence_len, max_ratio, phase='train', pad=0):\n",
        "\n",
        "        if phase == 'train':\n",
        "            self.img_dir = os.path.join(base_data_dir, 'train')\n",
        "            self.lbl_path = os.path.join(base_data_dir, 'gt.txt')\n",
        "        else:\n",
        "            self.img_dir = os.path.join(base_data_dir, 'test')\n",
        "            self.lbl_path = os.path.join(base_data_dir, 'test_gt.txt')\n",
        "        self.lbl2id_map = lbl2id_map\n",
        "        self.pad = pad   # padding标识符的id，默认0\n",
        "        self.sequence_len = sequence_len    # 序列长度\n",
        "        self.max_ratio = max_ratio * 3      # 将宽拉长3倍\n",
        "\n",
        "        self.imgs_list = []\n",
        "        self.lbls_list = []\n",
        "        with open(self.lbl_path, 'r') as reader:\n",
        "            for line in reader:\n",
        "                items = line.rstrip().split(',')\n",
        "                img_name = items[0]\n",
        "                lbl_str = items[1].strip()[1:-1]\n",
        "\n",
        "                self.imgs_list.append(img_name)\n",
        "                self.lbls_list.append(lbl_str)\n",
        "\n",
        "        # 定义随机颜色变换\n",
        "        self.color_trans = transforms.ColorJitter(0.1, 0.1, 0.1)\n",
        "        # 定义 Normalize\n",
        "        self.trans_Normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
        "        ]) \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" \n",
        "        获取对应index的图像和ground truth label，并视情况进行数据增强\n",
        "        \"\"\"\n",
        "        img_name = self.imgs_list[index]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        lbl_str = self.lbls_list[index]\n",
        "\n",
        "        # ----------------\n",
        "        # 图片预处理\n",
        "        # ----------------\n",
        "        # load image\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # 对图片进行大致等比例的缩放\n",
        "        # 将高缩放到32，宽大致等比例缩放，但要被32整除\n",
        "        w, h = img.size\n",
        "        ratio = round((w / h) * 3)   # 将宽拉长3倍，然后四舍五入\n",
        "        if ratio == 0:\n",
        "            ratio = 1\n",
        "        if ratio > self.max_ratio:\n",
        "            ratio = self.max_ratio\n",
        "        h_new = 32\n",
        "        w_new = h_new * ratio\n",
        "        img_resize = img.resize((w_new, h_new), Image.BILINEAR)\n",
        "\n",
        "        # 对图片右半边进行padding，使得宽/高比例固定=self.max_ratio\n",
        "        img_padd = Image.new('RGB', (32*self.max_ratio, 32), (0,0,0))\n",
        "        img_padd.paste(img_resize, (0, 0))\n",
        "\n",
        "        # 随机颜色变换\n",
        "        img_input = self.color_trans(img_padd)\n",
        "        # Normalize\n",
        "        img_input = self.trans_Normalize(img_input)\n",
        "\n",
        "        # ----------------\n",
        "        # label处理\n",
        "        # ----------------\n",
        "\n",
        "        # 构造encoder的mask\n",
        "        encode_mask = [1] * ratio + [0] * (self.max_ratio - ratio)\n",
        "        encode_mask = torch.tensor(encode_mask)\n",
        "        encode_mask = (encode_mask != 0).unsqueeze(0)\n",
        "\n",
        "        # 构造ground truth label\n",
        "        gt = []\n",
        "        gt.append(1)    # 先添加句子起始符\n",
        "        for lbl in lbl_str:\n",
        "            gt.append(self.lbl2id_map[lbl])\n",
        "        gt.append(2)\n",
        "        for i in range(len(lbl_str), self.sequence_len):   # 除去起始符终止符，lbl长度为sequence_len，剩下的padding\n",
        "            gt.append(0)\n",
        "        # 截断为预设的最大序列长度\n",
        "        gt = gt[:self.sequence_len+2]\n",
        "\n",
        "        # decoder的输入\n",
        "        decode_in = gt[:-1]\n",
        "        decode_in = torch.tensor(decode_in)\n",
        "        # decoder的输出\n",
        "        decode_out = gt[1:]\n",
        "        decode_out = torch.tensor(decode_out)\n",
        "        # decoder的mask \n",
        "        decode_mask = self.make_std_mask(decode_in, self.pad)\n",
        "        # 有效tokens数\n",
        "        ntokens = (decode_out != self.pad).data.sum()\n",
        "\n",
        "        return img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens \n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"\"\"\n",
        "        Create a mask to hide padding and future words.\n",
        "        padd 和 future words 均在mask中用0表示\n",
        "        \"\"\"\n",
        "        tgt_mask = (tgt != pad)\n",
        "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        tgt_mask = tgt_mask.squeeze(0)   # subsequent返回值的shape是(1, N, N)\n",
        "        return tgt_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_list)\n",
        "\n",
        "\n",
        "# Model Architecture\n",
        "class OCR_EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. \n",
        "    Base for this and many other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, src_position, tgt_embed, generator):\n",
        "        super(OCR_EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed    # input embedding module(input embedding + positional encode)\n",
        "        self.src_position = src_position\n",
        "        self.tgt_embed = tgt_embed    # ouput embedding module\n",
        "        self.generator = generator    # output generation module\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        memory = self.encode(src, src_mask)\n",
        "        res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
        "        return res\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        # feature extract\n",
        "        src_embedds = self.src_embed(src)\n",
        "        # 将src_embedds由shape(bs, model_dim, 1, max_ratio) 处理为transformer期望的输入shape(bs, 时间步, model_dim)\n",
        "        src_embedds = src_embedds.squeeze(-2)\n",
        "        src_embedds = src_embedds.permute(0, 2, 1)\n",
        "\n",
        "        # position encode\n",
        "        src_embedds = self.src_position(src_embedds)\n",
        "\n",
        "        return self.encoder(src_embedds, src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        target_embedds = self.tgt_embed(tgt)\n",
        "        return self.decoder(target_embedds, memory, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "def make_ocr_model(tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"\"\"\n",
        "    构建模型\n",
        "    params:\n",
        "        tgt_vocab: 输出的词典大小(82)\n",
        "        N: 编码器和解码器堆叠基础模块的个数\n",
        "        d_model: 模型中embedding的size，默认512\n",
        "        d_ff: FeedForward Layer层中embedding的size，默认2048\n",
        "        h: MultiHeadAttention中多头的个数，必须被d_model整除\n",
        "        dropout:\n",
        "    \"\"\"\n",
        "    c = copy.deepcopy\n",
        "\n",
        "    backbone = models.resnet18(pretrained=True)\n",
        "    backbone = nn.Sequential(*list(backbone.children())[:-2])    # 去掉最后两个层 (global average pooling and fc layer)\n",
        "\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "    model = OCR_EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        backbone,\n",
        "        c(position),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for child in model.children():\n",
        "        if child is backbone:\n",
        "            # 将backbone的权重设为不计算梯度\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "            # 预训练好的backbone不进行随机初始化，其余模块进行随机初始化\n",
        "            continue\n",
        "        for p in child.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_epoch(data_loader, model, loss_compute, device=None):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        #if device == \"cuda\":\n",
        "        #    batch.to_device(device) \n",
        "        img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "        img_input = img_input.to(device)                        \n",
        "        encode_mask = encode_mask.to(device)                                \n",
        "        decode_in = decode_in.to(device)                                \n",
        "        decode_out = decode_out.to(device)                    \n",
        "        decode_mask = decode_mask.to(device)\n",
        "        ntokens = torch.sum(ntokens).to(device)\n",
        "\n",
        "        out = model.forward(img_input, decode_in, encode_mask, decode_mask)\n",
        "\n",
        "        loss = loss_compute(out, decode_out, ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += ntokens\n",
        "        tokens += ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens\n",
        "\n",
        "\n",
        "# greedy decode\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol, end_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    # ys代表目前已生成的序列，最初为仅包含一个起始符的序列，不断将预测结果追加到序列最后\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data).long()\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        next_word = torch.ones(1, 1).type_as(src.data).fill_(next_word).long()\n",
        "        ys = torch.cat([ys, next_word], dim=1)\n",
        "\n",
        "        next_word = int(next_word)\n",
        "        if next_word == end_symbol:\n",
        "            break\n",
        "        #ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    ys = ys[0, 1:]\n",
        "    return ys\n",
        "\n",
        "\n",
        "def judge_is_correct(pred, label):\n",
        "    # 判断模型预测结果和label是否一致\n",
        "    pred_len = pred.shape[0]\n",
        "    label = label[:pred_len]\n",
        "    is_correct = 1 if label.equal(pred) else 0\n",
        "    return is_correct"
      ],
      "metadata": {
        "id": "EeSD_sBL9Nxg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the warmup scheduler as a :class:`dict`.\n",
        "        It contains an entry for every variable in self.__dict__ which\n",
        "        is not the optimizer.\n",
        "        \"\"\"\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "    \n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the warmup scheduler's state.\n",
        "        Arguments:\n",
        "            state_dict (dict): warmup scheduler state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state_dict) \n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "metadata": {
        "id": "6-fOVb5d_OgI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO set parameters\n",
        "base_data_dir = '/content/drive/MyDrive/icdar15'    # 数据集根目录，请将数据下载到此位置\n",
        "device = torch.device('cpu')\n",
        "nrof_epochs = 300\n",
        "batch_size = 64\n",
        "model_save_path = './log/ex1_ocr_model.pth'\n",
        "\n",
        "# 读取label-id映射关系记录文件\n",
        "lbl2id_map_path = os.path.join(base_data_dir, 'lbl2id_map.txt')\n",
        "lbl2id_map, id2lbl_map = load_lbl2id_map(lbl2id_map_path)\n",
        "\n",
        "# 统计数据集中出现的所有的label中包含字符最多的有多少字符，数据集构造gt信息需要用到\n",
        "train_lbl_path = os.path.join(base_data_dir, 'gt.txt')\n",
        "valid_lbl_path = os.path.join(base_data_dir, 'test_gt.txt')\n",
        "train_max_label_len = statistics_max_len_label(train_lbl_path)\n",
        "valid_max_label_len = statistics_max_len_label(valid_lbl_path)\n",
        "sequence_len = max(train_max_label_len, valid_max_label_len)   # 数据集中字符数最多的一个case作为制作的gt的sequence_len\n",
        "\n",
        "# 构造 dataloader\n",
        "max_ratio = 8    # 图片预处理时 宽/高的最大值，不超过就保比例resize，超过会强行压缩\n",
        "train_dataset = Recognition_Dataset(base_data_dir, lbl2id_map, sequence_len, max_ratio, 'train', pad=0)\n",
        "valid_dataset = Recognition_Dataset(base_data_dir, lbl2id_map, sequence_len, max_ratio, 'test', pad=0)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    num_workers=4)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=False,\n",
        "                                    num_workers=4)  \n",
        "\n",
        "# build model\n",
        "# use transformer as ocr recognize model\n",
        "tgt_vocab = len(lbl2id_map.keys())\n",
        "d_model = 512\n",
        "ocr_model = make_ocr_model(tgt_vocab, N=5, d_model=d_model, d_ff=2048, h=8, dropout=0.1)\n",
        "ocr_model.to(device)\n",
        "\n",
        "# train prepare\n",
        "criterion = LabelSmoothing(size=tgt_vocab, padding_idx=0, smoothing=0.0)\n",
        "\n",
        "# choose a optimizer\n",
        "optimizer = torch.optim.Adam(ocr_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "#optimizer = torch.optim.SGD(ocr_model.parameters(), lr=0.001, momentum=0.9)\n",
        "model_opt = NoamOpt(d_model,200,optimizer)\n",
        "\n",
        "\n",
        "for epoch in range(nrof_epochs):\n",
        "  print(f\"\\nepoch {epoch}\")\n",
        "\n",
        "  print(\"train...\")\n",
        "  ocr_model.train()\n",
        "  loss_compute = SimpleLossCompute(ocr_model.generator, criterion, model_opt)\n",
        "  #loss_compute = SimpleLossCompute(ocr_model.generator, criterion, optimizer)\n",
        "  train_mean_loss = run_epoch(train_loader, ocr_model, loss_compute, device)\n",
        "\n",
        "  if epoch % 20 == 0:\n",
        "    print(\"valid...\")\n",
        "    ocr_model.eval()\n",
        "    valid_loss_compute = SimpleLossCompute(ocr_model.generator, criterion, None)\n",
        "    valid_mean_loss = run_epoch(valid_loader, ocr_model, valid_loss_compute, device)\n",
        "    print(f\"valid loss: {valid_mean_loss}\")\n",
        "\n",
        "# save model\n",
        "torch.save(ocr_model.state_dict(), model_save_path)\n",
        "\n",
        "# save model\n",
        "torch.save(ocr_model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMt-LR5N9gKh",
        "outputId": "cebef533-bcd4-4b6c-f7e1-fb5f8ea17ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "train...\n",
            "Epoch Step: 1 Loss: 4.850827 Tokens per Sec: 37.065262\n",
            "Epoch Step: 51 Loss: 2.854465 Tokens per Sec: 42.415791\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 2.871195 Tokens per Sec: 16.539686\n",
            "valid loss: 2.7329490184783936\n",
            "\n",
            "epoch 1\n",
            "train...\n",
            "Epoch Step: 1 Loss: 4.715374 Tokens per Sec: 36.040276\n",
            "Epoch Step: 51 Loss: 2.817122 Tokens per Sec: 40.043793\n",
            "\n",
            "epoch 2\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.563766 Tokens per Sec: 35.349197\n",
            "Epoch Step: 51 Loss: 2.728527 Tokens per Sec: 39.267166\n",
            "\n",
            "epoch 3\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.662421 Tokens per Sec: 31.443756\n",
            "Epoch Step: 51 Loss: 2.544282 Tokens per Sec: 35.474258\n",
            "\n",
            "epoch 4\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.590855 Tokens per Sec: 30.979532\n",
            "Epoch Step: 51 Loss: 2.546010 Tokens per Sec: 35.175621\n",
            "\n",
            "epoch 5\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.639744 Tokens per Sec: 32.460304\n",
            "Epoch Step: 51 Loss: 2.644145 Tokens per Sec: 35.155739\n",
            "\n",
            "epoch 6\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.601664 Tokens per Sec: 30.634779\n",
            "Epoch Step: 51 Loss: 2.446588 Tokens per Sec: 36.092007\n",
            "\n",
            "epoch 7\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.474649 Tokens per Sec: 30.726652\n",
            "Epoch Step: 51 Loss: 2.578128 Tokens per Sec: 36.797024\n",
            "\n",
            "epoch 8\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.399706 Tokens per Sec: 32.932491\n",
            "Epoch Step: 51 Loss: 2.335406 Tokens per Sec: 36.631905\n",
            "\n",
            "epoch 9\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.381890 Tokens per Sec: 31.733049\n",
            "Epoch Step: 51 Loss: 2.417150 Tokens per Sec: 37.066097\n",
            "\n",
            "epoch 10\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.315753 Tokens per Sec: 30.430387\n",
            "Epoch Step: 51 Loss: 2.350831 Tokens per Sec: 36.663265\n",
            "\n",
            "epoch 11\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.381728 Tokens per Sec: 34.996872\n",
            "Epoch Step: 51 Loss: 2.323637 Tokens per Sec: 36.722462\n",
            "\n",
            "epoch 12\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.416814 Tokens per Sec: 31.271057\n",
            "Epoch Step: 51 Loss: 2.409688 Tokens per Sec: 37.059052\n",
            "\n",
            "epoch 13\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.255898 Tokens per Sec: 29.819216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练结束，使用贪心的解码方式推理训练集和验证集，统计正确率\n",
        "ocr_model.eval()\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"greedy decode trainset\")\n",
        "total_img_num = 0\n",
        "total_correct_num = 0\n",
        "for batch_idx, batch in enumerate(train_loader):\n",
        "    img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "    img_input = img_input.to(device)                        \n",
        "    encode_mask = encode_mask.to(device)                                \n",
        "\n",
        "bs = img_input.shape[0]\n",
        "for i in range(bs):\n",
        "    cur_img_input = img_input[i].unsqueeze(0)\n",
        "    cur_encode_mask = encode_mask[i].unsqueeze(0)\n",
        "    cur_decode_out = decode_out[i]\n",
        "\n",
        "    pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=sequence_len, start_symbol=1, end_symbol=2)\n",
        "    pred_result = pred_result.cpu()\n",
        "\n",
        "    is_correct = judge_is_correct(pred_result, cur_decode_out)\n",
        "    total_correct_num += is_correct\n",
        "    total_img_num += 1\n",
        "    if not is_correct:\n",
        "        # 预测错误的case进行打印\n",
        "        print(\"----\")\n",
        "        print(cur_decode_out)\n",
        "        print(pred_result)\n",
        "total_correct_rate = total_correct_num / total_img_num * 100\n",
        "print(f\"total correct rate of trainset: {total_correct_rate}%\")"
      ],
      "metadata": {
        "id": "32UwCsat_dgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"greedy decode validset\")\n",
        "total_img_num = 0\n",
        "total_correct_num = 0\n",
        "for batch_idx, batch in enumerate(valid_loader):\n",
        "    img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "    img_input = img_input.to(device)                        \n",
        "    encode_mask = encode_mask.to(device)                                \n",
        "\n",
        "bs = img_input.shape[0]\n",
        "for i in range(bs):\n",
        "    cur_img_input = img_input[i].unsqueeze(0)\n",
        "    cur_encode_mask = encode_mask[i].unsqueeze(0)\n",
        "    cur_decode_out = decode_out[i]\n",
        "\n",
        "    pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=sequence_len, start_symbol=1, end_symbol=2)\n",
        "    pred_result = pred_result.cpu()\n",
        "\n",
        "    is_correct = judge_is_correct(pred_result, cur_decode_out)\n",
        "    total_correct_num += is_correct\n",
        "    total_img_num += 1\n",
        "    if not is_correct:\n",
        "        # 预测错误的case进行打印\n",
        "        print(\"----\")\n",
        "        print(cur_decode_out)\n",
        "        print(pred_result)\n",
        "total_correct_rate = total_correct_num / total_img_num * 100\n",
        "print(f\"total correct rate of validset: {total_correct_rate}%\")"
      ],
      "metadata": {
        "id": "Zw4EhMLp9noL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S4g5VGFt_Rqz8EICt8i8pP5_IKyUnQbC",
      "authorship_tag": "ABX9TyP7QsCr21lAwbwdrg1g6DNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6655483fc9924afcaf787966d456f132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f24058087e504c14a3c215ac87ca4ea2",
              "IPY_MODEL_65839620a5724a9095a37a745fec0d5e",
              "IPY_MODEL_fa7f93b57d0c40cbaaeef8698bd73db8"
            ],
            "layout": "IPY_MODEL_cc475b1fcb574863b076d38e26439b60"
          }
        },
        "f24058087e504c14a3c215ac87ca4ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0f513a4c752414497b850171084755a",
            "placeholder": "​",
            "style": "IPY_MODEL_7d5a988626ac4edfa04fd63af0cf48f9",
            "value": "100%"
          }
        },
        "65839620a5724a9095a37a745fec0d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e9af481cc874ff28767c5f550c5a4db",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ca0a8e5b42e470b8dad42a11fc6e26f",
            "value": 46830571
          }
        },
        "fa7f93b57d0c40cbaaeef8698bd73db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d77b165f86bb452392050163699fd19a",
            "placeholder": "​",
            "style": "IPY_MODEL_1b6e7e6163cb4221bfc66b10d0a22ae2",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 191MB/s]"
          }
        },
        "cc475b1fcb574863b076d38e26439b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0f513a4c752414497b850171084755a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d5a988626ac4edfa04fd63af0cf48f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e9af481cc874ff28767c5f550c5a4db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ca0a8e5b42e470b8dad42a11fc6e26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d77b165f86bb452392050163699fd19a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b6e7e6163cb4221bfc66b10d0a22ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammadsulton1/ML/blob/main/ViT_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "zipfile = '/content/drive/MyDrive/autoriaNumberplateOcrRu-2021-09-01.tar.gz'\n",
        "if zipfile.endswith(\"tar.gz\"):\n",
        "    tar = tarfile.open(zipfile, \"r:gz\")\n",
        "elif zipfile.endswith(\"tar\"):\n",
        "    tar = tarfile.open(zipfile, \"r:\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "et6bwmOVHO2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import albumentations as A\n",
        "import time"
      ],
      "metadata": {
        "id": "gLpk-DPuMuiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def json_to_txt(path_name_to_save,path):\n",
        "  for name in os.listdir(path):\n",
        "    with open(os.path.join(path, name), 'r') as f:\n",
        "      data = json.loads(f.read())\n",
        "      img_name = data.get('name') + '.png'\n",
        "      description = data.get('description')\n",
        "      with open(path_name_to_save + \".txt\", \"a\") as text_file:\n",
        "        text_file.write('{},{}'.format(img_name, description + '\\n'))\n",
        "        text_file.close()"
      ],
      "metadata": {
        "id": "ayzGKzsz7_BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_to_txt('train_label','OCR_Licplate/train/ann')"
      ],
      "metadata": {
        "id": "mzUFVEMfNGz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_to_txt('valid_label','OCR_Licplate/val/ann')"
      ],
      "metadata": {
        "id": "T5ulZbKeNeiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_to_txt('test_label','OCR_Licplate/test/ann')"
      ],
      "metadata": {
        "id": "Sg1dnljXciwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def statistics_label_cnt(lbl_path, lbl_cnt_map):\n",
        "    with open(lbl_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split(',')\n",
        "            img_name = items[0]\n",
        "            lbl_str = items[1].strip()\n",
        "            for lbl in lbl_str:\n",
        "                if lbl not in lbl_cnt_map.keys():\n",
        "                    lbl_cnt_map[lbl] = 1\n",
        "                else:\n",
        "                    lbl_cnt_map[lbl] += 1\n",
        "\n",
        "\n",
        "def statistics_max_len_label(lbl_path):\n",
        "    max_len = -1\n",
        "    with open(lbl_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split(',')\n",
        "            img_name = items[0]\n",
        "            lbl_str = items[1].strip()\n",
        "            lbl_len = len(lbl_str)\n",
        "            max_len = max_len if max_len > lbl_len else lbl_len\n",
        "    return max_len\n",
        "\n",
        "\n",
        "def load_lbl2id_map(lbl2id_map_path):\n",
        "    lbl2id_map = dict()\n",
        "    id2lbl_map = dict()\n",
        "    with open(lbl2id_map_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split(' ')\n",
        "            label = items[0]\n",
        "            cur_id = int(items[1])\n",
        "            lbl2id_map[label] = cur_id\n",
        "            id2lbl_map[cur_id] = label\n",
        "    return lbl2id_map, id2lbl_map"
      ],
      "metadata": {
        "id": "bkjNC1o-NVWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_data_dir = '/content/OCR_Licplate'\n",
        "\n",
        "train_img_dir = os.path.join(base_data_dir, 'train')\n",
        "valid_img_dir = os.path.join(base_data_dir, 'val')\n",
        "train_lbl_path = 'train_label.txt'\n",
        "valid_lbl_path = 'valid_label.txt'\n",
        "lbl2id_map_path = 'lbl2id_map.txt'\n",
        "\n",
        "train_max_label_len = statistics_max_len_label(train_lbl_path)\n",
        "valid_max_label_len = statistics_max_len_label(valid_lbl_path)\n",
        "max_label_len = max(train_max_label_len, valid_max_label_len)\n",
        "print(f\"max len {max_label_len}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DH886dAOGrb",
        "outputId": "14446511-04b3-4d58-ba04-1c2d3846ace5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max len 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "PzSKhk7_QFZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
        "\n",
        "\n",
        "# class SimpleLossCompute:\n",
        "#     \"A simple loss compute and train function.\"\n",
        "#     def __init__(self, generator, criterion, opt=None):\n",
        "#         self.generator = generator\n",
        "#         self.criterion = criterion\n",
        "#         self.opt = opt\n",
        "        \n",
        "#     def __call__(self, x, y, norm):\n",
        "#         x = self.generator(x)\n",
        "#         x_ = x.contiguous().view(-1, x.size(-1))\n",
        "#         y_ = y.contiguous().view(-1)\n",
        "#         loss = self.criterion(x_, y_)\n",
        "#         loss /= norm\n",
        "#         loss.backward()\n",
        "#         if self.opt is not None:\n",
        "#             self.opt.step()\n",
        "#             self.opt.zero_grad()\n",
        "#         return loss.item() * norm"
      ],
      "metadata": {
        "id": "Aeqm6Sc4Qj05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#simplelosscompute"
      ],
      "metadata": {
        "id": "vTUvOyxI-D-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        \"\"\"\n",
        "        norm: loss的归一化系数，用batch中所有有效token数即可\n",
        "        \"\"\"\n",
        "        x = self.generator(x)\n",
        "        x_ = x.contiguous().view(-1, x.size(-1))\n",
        "        y_ = y.contiguous().view(-1)\n",
        "        loss = self.criterion(x_, y_)\n",
        "        loss /= norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        #return loss.data[0] * norm  # TODO\n",
        "        return loss.item() * norm"
      ],
      "metadata": {
        "id": "lFYxf342-DbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder\n",
        "    The encoder is composed of a stack of N=6 identical layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# We employ a residual connection around each of the two sub-layers, followed by layer normalization\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, feature_size, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(feature_size))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(feature_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        sublayer_out = sublayer(x)\n",
        "        sublayer_out = self.dropout(sublayer_out)\n",
        "        x_norm = x + self.norm(sublayer_out)\n",
        "        return x_norm\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        # attention sub layer\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        # feed forward sub layer\n",
        "        z = self.sublayer[1](x, self.feed_forward)\n",
        "        return z\n",
        "\n",
        "\n",
        "# Decoder\n",
        "# The decoder is also composed of a stack of N=6 identical layers.\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "\n",
        "# Attention\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "\n",
        "# Embeddings and Softmax\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedds = self.lut(x)\n",
        "        return embedds * math.sqrt(self.d_model)\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "1qAJw5S2QFhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the warmup scheduler as a :class:`dict`.\n",
        "        It contains an entry for every variable in self.__dict__ which\n",
        "        is not the optimizer.\n",
        "        \"\"\"\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "    \n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the warmup scheduler's state.\n",
        "        Arguments:\n",
        "            state_dict (dict): warmup scheduler state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state_dict) \n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "metadata": {
        "id": "lA7e4iEEXvju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OCR_BY_TRANSFORMER"
      ],
      "metadata": {
        "id": "LHD5dyPoRxtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset_root_dir, lbl2id_map, sequence_len, phase='train', pad=0):\n",
        "        if phase == 'train':\n",
        "            self.img_dir = os.path.join(dataset_root_dir, 'train/img/')\n",
        "            self.lbl_path = 'train_label.txt'\n",
        "        elif phase == 'valid':\n",
        "            self.img_dir = os.path.join(dataset_root_dir, 'val/img/')\n",
        "            self.lbl_path = 'valid_label.txt'\n",
        "        else:\n",
        "            self.img_dir = os.path.join(dataset_root_dir, 'test/img/')\n",
        "            self.lbl_path = 'test_label.txt'\n",
        "\n",
        "        self.lbl2id_map = lbl2id_map\n",
        "        self.pad = pad\n",
        "        self.sequence_len = sequence_len\n",
        "\n",
        "\n",
        "        self.resize = transforms.Resize((32, 160))\n",
        "\n",
        "\n",
        "        self.imgs_list = []\n",
        "        self.lbls_list = []\n",
        "\n",
        "        with open(self.lbl_path, 'r') as reader:\n",
        "            for line in reader:\n",
        "                items = line.rstrip().split(',')\n",
        "                img_name = items[0]\n",
        "                lbl_str = items[1]\n",
        "\n",
        "                self.imgs_list.append(img_name)\n",
        "                self.lbls_list.append(lbl_str)\n",
        "\n",
        "        # self.color_trans = transforms.ColorJitter(0.1, 0.1, 0.1)\n",
        "        self.trans_Normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.imgs_list[index]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        lbl_str = self.lbls_list[index]\n",
        "\n",
        "        # load image\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        h_new = int(32)\n",
        "        w_new = int(160)\n",
        "        img_resize = img.resize((w_new, h_new), Image.BILINEAR)\n",
        "        \n",
        "        img_input = self.trans_Normalize(img_resize)\n",
        "\n",
        "        # encode_mask = [1] * ratio + [0] * (self.max_ratio - ratio)\n",
        "        encode_mask = [1] * int(56) + [0] * int(24) #112 + 112 (66 + 14) 56 + 24\n",
        "        encode_mask = torch.tensor(encode_mask)\n",
        "        encode_mask = (encode_mask != 0).unsqueeze(0)\n",
        "\n",
        "        # ground truth label\n",
        "        gt = []\n",
        "        gt.append(1)\n",
        "        for lbl in lbl_str:\n",
        "            gt.append(self.lbl2id_map[lbl])\n",
        "        gt.append(2)\n",
        "        for i in range(len(lbl_str), self.sequence_len):\n",
        "            gt.append(0)\n",
        "        gt = gt[:self.sequence_len + 2]\n",
        "\n",
        "        # decoder\n",
        "        decode_in = gt[:-1]\n",
        "        decode_in = torch.tensor(decode_in)\n",
        "        # decoder\n",
        "        decode_out = gt[1:]\n",
        "        decode_out = torch.tensor(decode_out)\n",
        "        # decoder_mask\n",
        "        decode_mask = self.make_std_mask(decode_in, self.pad)\n",
        "        # tokens\n",
        "        ntokens = (decode_out != self.pad).data.sum()\n",
        "\n",
        "        return img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens\n",
        "\n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "\n",
        "        tgt_mask = (tgt != pad)\n",
        "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        tgt_mask = tgt_mask.squeeze(0)  # subsequent (1, N, N)\n",
        "        return tgt_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_list)"
      ],
      "metadata": {
        "id": "8E6HogORRs94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def see_plot(pict, color='gray', size=(4,4), title='None'):\n",
        "    plt.figure(figsize=size)\n",
        "    plt.imshow(pict, cmap=color)\n",
        "    plt.title(title)\n",
        "    #plt.grid()\n",
        "    #plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "def plot_samples_on_epoch(samples, nrow=4, chanels=3, size=(16,16), title='None'):\n",
        "    grid_img = torchvision.utils.make_grid(samples, nrow=nrow)\n",
        "    if chanels==1:\n",
        "         see_plot(grid_img.permute(1, 2, 0)*255, size=size, title=title)\n",
        "    else:\n",
        "         see_plot(grid_img.permute(1, 2, 0), size=size, title=title)\n",
        "    #see_plot(grid_img, size=size, title=title)"
      ],
      "metadata": {
        "id": "IZdP8xcplZ9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model Architecture\n",
        "# class OCR_EncoderDecoder(nn.Module):\n",
        "#     \"\"\"\n",
        "#     A standard Encoder-Decoder architecture. \n",
        "#     Base for this and many other models.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, decoder, src_embed, src_position, tgt_embed, generator):\n",
        "#         super(OCR_EncoderDecoder, self).__init__()\n",
        "#         self.decoder = decoder\n",
        "#         self.src_embed = src_embed    # input embedding module(input embedding + positional encode)\n",
        "#         self.src_position = src_position\n",
        "#         self.tgt_embed = tgt_embed    # ouput embedding module\n",
        "#         self.generator = generator    # output generation module\n",
        "        \n",
        "#     def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "#         \"Take in and process masked src and target sequences.\"\n",
        "#         memory = self.encode(src, src_mask)\n",
        "#         res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
        "#         return res\n",
        "    \n",
        "#     def encode(self, src, src_mask):\n",
        "#         # feature extract\n",
        "#         src_embedds = self.src_embed(src)\n",
        "#         src_embedds = torch.flatten(src_embedds,2,3)\n",
        "#         src_embedds = src_embedds.permute(0, 2, 1)\n",
        "\n",
        "#         # position encode\n",
        "#         # src_embedds = self.src_position(src_embedds)\n",
        "#         return src_embedds\n",
        "\n",
        "#         # return self.encoder(src_embedds, src_mask)\n",
        "    \n",
        "#     def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "#         target_embedds = self.tgt_embed(tgt)\n",
        "#         return self.decoder(target_embedds, memory, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "# def make_ocr_model(tgt_vocab, N=3, d_model=128, d_ff=128*4, h=4, dropout=0.1):\n",
        "#     c = copy.deepcopy\n",
        "\n",
        "#     backbone = models.resnet18(pretrained=True)\n",
        "#     backbone = nn.Sequential(*list(backbone.children())[:-4])\n",
        "\n",
        "#     attn = MultiHeadedAttention(h, d_model)\n",
        "#     ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "#     position = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "#     model = OCR_EncoderDecoder(\n",
        "#         Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "#         backbone,\n",
        "#         c(position),\n",
        "#         nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "#         Generator(d_model, tgt_vocab))\n",
        "    \n",
        "#     for child in model.children():\n",
        "#         if child is backbone:\n",
        "#             for param in child.parameters():\n",
        "#                 param.requires_grad = False\n",
        "#             continue\n",
        "#         for p in child.parameters():\n",
        "#             if p.dim() > 1:\n",
        "#                 nn.init.xavier_uniform_(p)\n",
        "#     return model\n"
      ],
      "metadata": {
        "id": "534Dz8cgSHZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "class OCR_EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. \n",
        "    Base for this and many other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, src_position, tgt_embed, generator):\n",
        "        super(OCR_EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed    # input embedding module(input embedding + positional encode)\n",
        "        self.src_position = src_position\n",
        "        self.tgt_embed = tgt_embed    # ouput embedding module\n",
        "        self.generator = generator    # output generation module\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        memory = self.encode(src, src_mask)\n",
        "        res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
        "        return res\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        # feature extract\n",
        "        src_embedds = self.src_embed(src)\n",
        "        #############\n",
        "        #src_embedds = src_embedds.squeeze(-2)\n",
        "        src_embedds = torch.flatten(src_embedds,2,3)\n",
        "        src_embedds = src_embedds.permute(0, 2, 1)\n",
        "\n",
        "        # position encode\n",
        "        src_embedds = self.src_position(src_embedds)\n",
        "\n",
        "        return self.encoder(src_embedds, src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        target_embedds = self.tgt_embed(tgt)\n",
        "        return self.decoder(target_embedds, memory, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "def make_ocr_model(tgt_vocab, N=3, d_model=128, d_ff=128*4, h=4, dropout=0.1):\n",
        "\n",
        "    c = copy.deepcopy\n",
        "\n",
        "    backbone = models.resnet18(pretrained=True)\n",
        "    backbone = nn.Sequential(*list(backbone.children())[:-4])\n",
        "\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "    model = OCR_EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        backbone,\n",
        "        c(position),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    for child in model.children():\n",
        "        if child is backbone:\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "            continue\n",
        "        for p in child.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "metadata": {
        "id": "HeCKIz6rs4-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(data_loader, model, loss_compute, device=None):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        #if device == \"cuda\":\n",
        "        #    batch.to_device(device) \n",
        "        img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "        img_input = img_input.to(device)\n",
        "        encode_mask = encode_mask.to(device)                                \n",
        "        decode_in = decode_in.to(device)                                \n",
        "        decode_out = decode_out.to(device)                    \n",
        "        decode_mask = decode_mask.to(device)\n",
        "        ntokens = torch.sum(ntokens).to(device)\n",
        "\n",
        "        out = model.forward(img_input, decode_in, encode_mask, decode_mask)\n",
        "\n",
        "        loss = loss_compute(out, decode_out, ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += ntokens\n",
        "        tokens += ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens\n",
        "\n",
        "\n",
        "# greedy decode\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol, end_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data).long()\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        next_word = torch.ones(1, 1).type_as(src.data).fill_(next_word).long()\n",
        "        ys = torch.cat([ys, next_word], dim=1)\n",
        "\n",
        "        next_word = int(next_word)\n",
        "        if next_word == end_symbol:\n",
        "            break\n",
        "        #ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    ys = ys[0, 1:]\n",
        "    return ys\n",
        "\n",
        "\n",
        "# def check_correct(pred, label):\n",
        "#     pred_len = pred.shape[0]\n",
        "#     label = label[:pred_len]\n",
        "#     is_correct = 1 if label.equal(pred) else 0\n",
        "#     return is_correct\n",
        "\n",
        "def check_correct(pred, label):\n",
        "  label_non_zero = label.nonzero().shape[0]\n",
        "  label = label[:label_non_zero]\n",
        "  is_correct = 1 if label.equal(pred) else 0\n",
        "  return is_correct"
      ],
      "metadata": {
        "id": "0Oy2vO4QRtHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "nrof_epochs = 200\n",
        "batch_size = 512\n",
        "model_save_path = 'ex1_ocr_model.pth'\n",
        "seq_len = 10\n",
        "\n",
        "lbl2id_map, id2lbl_map = load_lbl2id_map('/content/lbl2id_map.txt')\n",
        "\n",
        "sequence_len = max(train_max_label_len, valid_max_label_len)\n",
        "\n",
        "#dataloader\n",
        "train_dataset = Data_Dataset(base_data_dir, lbl2id_map, seq_len, 'train', pad=0) #sequence_len\n",
        "valid_dataset = Data_Dataset(base_data_dir, lbl2id_map, seq_len, 'valid', pad=0)\n",
        "test_dataset = Data_Dataset(base_data_dir, lbl2id_map, seq_len, 'test', pad=0)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=True,\n",
        "                                      num_workers=4)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      num_workers=4)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      num_workers=4)"
      ],
      "metadata": {
        "id": "nSawTR_PSceA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model\n",
        "# use transformer as ocr recognize model\n",
        "tgt_vocab = len(lbl2id_map.keys())\n",
        "d_model = 128\n",
        "ocr_model = make_ocr_model(tgt_vocab, N=3, d_model=d_model, d_ff=128*4, h=4, dropout=0.15)\n",
        "ocr_model.to(device)\n",
        "\n",
        "# train prepare\n",
        "criterion = LabelSmoothing(size=tgt_vocab, padding_idx=0, smoothing=0.0)\n",
        "\n",
        "# choose a optimizer\n",
        "#optimizer = torch.optim.Adam(ocr_model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
        "optimizer = torch.optim.Adam(ocr_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "#optimizer = torch.optim.SGD(ocr_model.parameters(), lr=0.001, momentum=0.9)\n",
        "model_opt = NoamOpt(d_model,200,optimizer)"
      ],
      "metadata": {
        "id": "e-lSo8IJWuIA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "6655483fc9924afcaf787966d456f132",
            "f24058087e504c14a3c215ac87ca4ea2",
            "65839620a5724a9095a37a745fec0d5e",
            "fa7f93b57d0c40cbaaeef8698bd73db8",
            "cc475b1fcb574863b076d38e26439b60",
            "f0f513a4c752414497b850171084755a",
            "7d5a988626ac4edfa04fd63af0cf48f9",
            "7e9af481cc874ff28767c5f550c5a4db",
            "5ca0a8e5b42e470b8dad42a11fc6e26f",
            "d77b165f86bb452392050163699fd19a",
            "1b6e7e6163cb4221bfc66b10d0a22ae2"
          ]
        },
        "outputId": "044c7f21-6d71-4162-af0e-e0566b3496dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6655483fc9924afcaf787966d456f132"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(nrof_epochs):\n",
        "  print(f\"\\nepoch {epoch}\")\n",
        "\n",
        "  print(\"train...\")\n",
        "  ocr_model.train()\n",
        "  loss_compute = SimpleLossCompute(ocr_model.generator, criterion, model_opt)\n",
        "  #loss_compute = SimpleLossCompute(ocr_model.generator, criterion, optimizer)\n",
        "  train_mean_loss = run_epoch(train_loader, ocr_model, loss_compute, device)\n",
        "\n",
        "  if epoch % 20 == 0:\n",
        "    print(\"valid...\")\n",
        "    ocr_model.eval()\n",
        "    valid_loss_compute = SimpleLossCompute(ocr_model.generator, criterion, None)\n",
        "    valid_mean_loss = run_epoch(valid_loader, ocr_model, valid_loss_compute, device)\n",
        "    print(f\"valid loss: {valid_mean_loss}\")\n",
        "\n",
        "# save model\n",
        "torch.save(ocr_model.state_dict(), model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY9jpE94qNmT",
        "outputId": "c98baa2a-bca5-43b2-fe94-ac11187c8676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "train...\n",
            "Epoch Step: 1 Loss: 4.508730 Tokens per Sec: 737.568726\n",
            "Epoch Step: 51 Loss: 2.426189 Tokens per Sec: 6279.211426\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 1.979805 Tokens per Sec: 2946.541748\n",
            "valid loss: 1.9730536937713623\n",
            "\n",
            "epoch 1\n",
            "train...\n",
            "Epoch Step: 1 Loss: 2.537446 Tokens per Sec: 2280.020996\n",
            "Epoch Step: 51 Loss: 1.986830 Tokens per Sec: 6360.949219\n",
            "\n",
            "epoch 2\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.843843 Tokens per Sec: 2942.832275\n",
            "Epoch Step: 51 Loss: 1.425876 Tokens per Sec: 6214.872559\n",
            "\n",
            "epoch 3\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.921072 Tokens per Sec: 2814.475586\n",
            "Epoch Step: 51 Loss: 0.614039 Tokens per Sec: 6164.723145\n",
            "\n",
            "epoch 4\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.502140 Tokens per Sec: 2663.722900\n",
            "Epoch Step: 51 Loss: 0.411506 Tokens per Sec: 6109.266113\n",
            "\n",
            "epoch 5\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.422446 Tokens per Sec: 1890.618164\n",
            "Epoch Step: 51 Loss: 0.382465 Tokens per Sec: 6470.503418\n",
            "\n",
            "epoch 6\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.340315 Tokens per Sec: 2066.650146\n",
            "Epoch Step: 51 Loss: 0.303805 Tokens per Sec: 6421.336426\n",
            "\n",
            "epoch 7\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.282305 Tokens per Sec: 2878.911133\n",
            "Epoch Step: 51 Loss: 0.301879 Tokens per Sec: 6254.756836\n",
            "\n",
            "epoch 8\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.271174 Tokens per Sec: 2755.795654\n",
            "Epoch Step: 51 Loss: 0.282774 Tokens per Sec: 6060.967285\n",
            "\n",
            "epoch 9\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.251734 Tokens per Sec: 3132.985596\n",
            "Epoch Step: 51 Loss: 0.243574 Tokens per Sec: 6033.627930\n",
            "\n",
            "epoch 10\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.224495 Tokens per Sec: 2127.551758\n",
            "Epoch Step: 51 Loss: 0.215134 Tokens per Sec: 6208.497070\n",
            "\n",
            "epoch 11\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.218934 Tokens per Sec: 2062.470947\n",
            "Epoch Step: 51 Loss: 0.211664 Tokens per Sec: 6393.827637\n",
            "\n",
            "epoch 12\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.214242 Tokens per Sec: 2677.991699\n",
            "Epoch Step: 51 Loss: 0.231133 Tokens per Sec: 6319.489746\n",
            "\n",
            "epoch 13\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.194292 Tokens per Sec: 2877.287598\n",
            "Epoch Step: 51 Loss: 0.199827 Tokens per Sec: 6105.557129\n",
            "\n",
            "epoch 14\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.186956 Tokens per Sec: 2451.566650\n",
            "Epoch Step: 51 Loss: 0.170050 Tokens per Sec: 6183.969238\n",
            "\n",
            "epoch 15\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.178363 Tokens per Sec: 1801.041260\n",
            "Epoch Step: 51 Loss: 0.165656 Tokens per Sec: 6205.018066\n",
            "\n",
            "epoch 16\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.164576 Tokens per Sec: 2714.312012\n",
            "Epoch Step: 51 Loss: 0.158299 Tokens per Sec: 6256.375977\n",
            "\n",
            "epoch 17\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.161331 Tokens per Sec: 2879.067871\n",
            "Epoch Step: 51 Loss: 0.164084 Tokens per Sec: 6114.353516\n",
            "\n",
            "epoch 18\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.141868 Tokens per Sec: 2712.833252\n",
            "Epoch Step: 51 Loss: 0.153986 Tokens per Sec: 6174.096191\n",
            "\n",
            "epoch 19\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.137740 Tokens per Sec: 2335.652588\n",
            "Epoch Step: 51 Loss: 0.134942 Tokens per Sec: 6280.413086\n",
            "\n",
            "epoch 20\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.150815 Tokens per Sec: 1870.605103\n",
            "Epoch Step: 51 Loss: 0.142519 Tokens per Sec: 6374.677246\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.041998 Tokens per Sec: 1984.198364\n",
            "valid loss: 0.040515098720788956\n",
            "\n",
            "epoch 21\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.173229 Tokens per Sec: 2761.932617\n",
            "Epoch Step: 51 Loss: 0.120082 Tokens per Sec: 6058.538086\n",
            "\n",
            "epoch 22\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.135043 Tokens per Sec: 2558.362793\n",
            "Epoch Step: 51 Loss: 0.134672 Tokens per Sec: 6200.968262\n",
            "\n",
            "epoch 23\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.130253 Tokens per Sec: 1788.312134\n",
            "Epoch Step: 51 Loss: 0.143372 Tokens per Sec: 6383.668945\n",
            "\n",
            "epoch 24\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.123302 Tokens per Sec: 2280.523438\n",
            "Epoch Step: 51 Loss: 0.117581 Tokens per Sec: 6402.453613\n",
            "\n",
            "epoch 25\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.117452 Tokens per Sec: 2783.706787\n",
            "Epoch Step: 51 Loss: 0.139214 Tokens per Sec: 5968.526855\n",
            "\n",
            "epoch 26\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.121066 Tokens per Sec: 2639.277100\n",
            "Epoch Step: 51 Loss: 0.121244 Tokens per Sec: 6198.584473\n",
            "\n",
            "epoch 27\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.109460 Tokens per Sec: 2881.575195\n",
            "Epoch Step: 51 Loss: 0.130981 Tokens per Sec: 6067.822266\n",
            "\n",
            "epoch 28\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.111402 Tokens per Sec: 1920.008301\n",
            "Epoch Step: 51 Loss: 0.110444 Tokens per Sec: 6383.613281\n",
            "\n",
            "epoch 29\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.110052 Tokens per Sec: 2553.165527\n",
            "Epoch Step: 51 Loss: 0.120578 Tokens per Sec: 6425.813477\n",
            "\n",
            "epoch 30\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.118165 Tokens per Sec: 2694.051514\n",
            "Epoch Step: 51 Loss: 0.100650 Tokens per Sec: 6038.654297\n",
            "\n",
            "epoch 31\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.108674 Tokens per Sec: 2685.617676\n",
            "Epoch Step: 51 Loss: 0.101964 Tokens per Sec: 6124.587891\n",
            "\n",
            "epoch 32\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.105233 Tokens per Sec: 1800.040894\n",
            "Epoch Step: 51 Loss: 0.097867 Tokens per Sec: 5848.416016\n",
            "\n",
            "epoch 33\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.105227 Tokens per Sec: 2847.315674\n",
            "Epoch Step: 51 Loss: 0.102356 Tokens per Sec: 6065.887207\n",
            "\n",
            "epoch 34\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.097582 Tokens per Sec: 2695.443604\n",
            "Epoch Step: 51 Loss: 0.091923 Tokens per Sec: 6075.285645\n",
            "\n",
            "epoch 35\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.100556 Tokens per Sec: 2051.834717\n",
            "Epoch Step: 51 Loss: 0.080037 Tokens per Sec: 6300.706055\n",
            "\n",
            "epoch 36\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.099801 Tokens per Sec: 2261.723145\n",
            "Epoch Step: 51 Loss: 0.086403 Tokens per Sec: 6452.460938\n",
            "\n",
            "epoch 37\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.100663 Tokens per Sec: 2866.846191\n",
            "Epoch Step: 51 Loss: 0.094179 Tokens per Sec: 6430.959961\n",
            "\n",
            "epoch 38\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.106025 Tokens per Sec: 2714.516602\n",
            "Epoch Step: 51 Loss: 0.088926 Tokens per Sec: 6218.129395\n",
            "\n",
            "epoch 39\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.105293 Tokens per Sec: 2738.680176\n",
            "Epoch Step: 51 Loss: 0.099111 Tokens per Sec: 6126.104004\n",
            "\n",
            "epoch 40\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.100092 Tokens per Sec: 2709.962891\n",
            "Epoch Step: 51 Loss: 0.097759 Tokens per Sec: 6204.189453\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.027164 Tokens per Sec: 1880.956909\n",
            "valid loss: 0.026575664058327675\n",
            "\n",
            "epoch 41\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.099015 Tokens per Sec: 2921.471680\n",
            "Epoch Step: 51 Loss: 0.089436 Tokens per Sec: 6132.785645\n",
            "\n",
            "epoch 42\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.109860 Tokens per Sec: 2798.624268\n",
            "Epoch Step: 51 Loss: 0.103307 Tokens per Sec: 6109.400879\n",
            "\n",
            "epoch 43\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.070234 Tokens per Sec: 2319.916016\n",
            "Epoch Step: 51 Loss: 0.074895 Tokens per Sec: 6278.483398\n",
            "\n",
            "epoch 44\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.097162 Tokens per Sec: 1948.279541\n",
            "Epoch Step: 51 Loss: 0.091852 Tokens per Sec: 6435.268066\n",
            "\n",
            "epoch 45\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.083980 Tokens per Sec: 2807.362549\n",
            "Epoch Step: 51 Loss: 0.081254 Tokens per Sec: 6407.769531\n",
            "\n",
            "epoch 46\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.083005 Tokens per Sec: 2862.431396\n",
            "Epoch Step: 51 Loss: 0.098299 Tokens per Sec: 6164.008789\n",
            "\n",
            "epoch 47\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.093718 Tokens per Sec: 2811.953613\n",
            "Epoch Step: 51 Loss: 0.091905 Tokens per Sec: 6175.991211\n",
            "\n",
            "epoch 48\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.073451 Tokens per Sec: 2663.154053\n",
            "Epoch Step: 51 Loss: 0.085079 Tokens per Sec: 6184.414551\n",
            "\n",
            "epoch 49\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.077467 Tokens per Sec: 2456.216797\n",
            "Epoch Step: 51 Loss: 0.074635 Tokens per Sec: 6273.222168\n",
            "\n",
            "epoch 50\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.071576 Tokens per Sec: 1916.227661\n",
            "Epoch Step: 51 Loss: 0.056982 Tokens per Sec: 6427.833008\n",
            "\n",
            "epoch 51\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.087873 Tokens per Sec: 2456.444824\n",
            "Epoch Step: 51 Loss: 0.082027 Tokens per Sec: 6389.636230\n",
            "\n",
            "epoch 52\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.061005 Tokens per Sec: 2702.150635\n",
            "Epoch Step: 51 Loss: 0.095497 Tokens per Sec: 6371.290039\n",
            "\n",
            "epoch 53\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.077907 Tokens per Sec: 3019.240967\n",
            "Epoch Step: 51 Loss: 0.080561 Tokens per Sec: 6110.348145\n",
            "\n",
            "epoch 54\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.074711 Tokens per Sec: 2859.714355\n",
            "Epoch Step: 51 Loss: 0.086267 Tokens per Sec: 6148.557617\n",
            "\n",
            "epoch 55\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.078084 Tokens per Sec: 2918.791748\n",
            "Epoch Step: 51 Loss: 0.064799 Tokens per Sec: 6197.264648\n",
            "\n",
            "epoch 56\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.067732 Tokens per Sec: 2177.803223\n",
            "Epoch Step: 51 Loss: 0.079748 Tokens per Sec: 6342.319336\n",
            "\n",
            "epoch 57\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.074210 Tokens per Sec: 1960.913940\n",
            "Epoch Step: 51 Loss: 0.075595 Tokens per Sec: 6524.973145\n",
            "\n",
            "epoch 58\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.068812 Tokens per Sec: 2537.923096\n",
            "Epoch Step: 51 Loss: 0.071074 Tokens per Sec: 6495.207031\n",
            "\n",
            "epoch 59\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.069270 Tokens per Sec: 2693.518311\n",
            "Epoch Step: 51 Loss: 0.077890 Tokens per Sec: 6366.339844\n",
            "\n",
            "epoch 60\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.085242 Tokens per Sec: 2784.854736\n",
            "Epoch Step: 51 Loss: 0.077900 Tokens per Sec: 6185.198242\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.020292 Tokens per Sec: 2741.057129\n",
            "valid loss: 0.01959318295121193\n",
            "\n",
            "epoch 61\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.078005 Tokens per Sec: 2801.494629\n",
            "Epoch Step: 51 Loss: 0.073278 Tokens per Sec: 6403.125488\n",
            "\n",
            "epoch 62\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.060185 Tokens per Sec: 2806.737061\n",
            "Epoch Step: 51 Loss: 0.070684 Tokens per Sec: 6247.956055\n",
            "\n",
            "epoch 63\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.064214 Tokens per Sec: 2666.131348\n",
            "Epoch Step: 51 Loss: 0.058002 Tokens per Sec: 6271.148438\n",
            "\n",
            "epoch 64\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.073252 Tokens per Sec: 2806.015869\n",
            "Epoch Step: 51 Loss: 0.054642 Tokens per Sec: 6197.556641\n",
            "\n",
            "epoch 65\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.064282 Tokens per Sec: 2677.634766\n",
            "Epoch Step: 51 Loss: 0.068544 Tokens per Sec: 6169.133789\n",
            "\n",
            "epoch 66\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.068522 Tokens per Sec: 1824.540039\n",
            "Epoch Step: 51 Loss: 0.070438 Tokens per Sec: 6471.352051\n",
            "\n",
            "epoch 67\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.066382 Tokens per Sec: 1942.965210\n",
            "Epoch Step: 51 Loss: 0.065809 Tokens per Sec: 6534.434082\n",
            "\n",
            "epoch 68\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.064486 Tokens per Sec: 2302.785156\n",
            "Epoch Step: 51 Loss: 0.067390 Tokens per Sec: 6408.469727\n",
            "\n",
            "epoch 69\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.065308 Tokens per Sec: 2684.021973\n",
            "Epoch Step: 51 Loss: 0.065894 Tokens per Sec: 6334.792480\n",
            "\n",
            "epoch 70\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.067553 Tokens per Sec: 2801.534424\n",
            "Epoch Step: 51 Loss: 0.055564 Tokens per Sec: 6088.967285\n",
            "\n",
            "epoch 71\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.060734 Tokens per Sec: 2799.486816\n",
            "Epoch Step: 51 Loss: 0.057184 Tokens per Sec: 6152.690430\n",
            "\n",
            "epoch 72\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.047137 Tokens per Sec: 2730.806396\n",
            "Epoch Step: 51 Loss: 0.057559 Tokens per Sec: 6151.972168\n",
            "\n",
            "epoch 73\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.055760 Tokens per Sec: 1910.009521\n",
            "Epoch Step: 51 Loss: 0.065987 Tokens per Sec: 6264.990234\n",
            "\n",
            "epoch 74\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.056356 Tokens per Sec: 2748.089111\n",
            "Epoch Step: 51 Loss: 0.066444 Tokens per Sec: 6357.518555\n",
            "\n",
            "epoch 75\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.068470 Tokens per Sec: 2709.089844\n",
            "Epoch Step: 51 Loss: 0.080035 Tokens per Sec: 6142.210449\n",
            "\n",
            "epoch 76\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.060040 Tokens per Sec: 2797.113770\n",
            "Epoch Step: 51 Loss: 0.064121 Tokens per Sec: 6204.254883\n",
            "\n",
            "epoch 77\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.059643 Tokens per Sec: 2785.351074\n",
            "Epoch Step: 51 Loss: 0.054660 Tokens per Sec: 6146.338379\n",
            "\n",
            "epoch 78\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.061325 Tokens per Sec: 1784.508301\n",
            "Epoch Step: 51 Loss: 0.058493 Tokens per Sec: 6257.463379\n",
            "\n",
            "epoch 79\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.055431 Tokens per Sec: 2609.782227\n",
            "Epoch Step: 51 Loss: 0.059532 Tokens per Sec: 6123.532227\n",
            "\n",
            "epoch 80\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.050469 Tokens per Sec: 2846.052734\n",
            "Epoch Step: 51 Loss: 0.051846 Tokens per Sec: 6036.124512\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.019048 Tokens per Sec: 2578.738281\n",
            "valid loss: 0.018502606078982353\n",
            "\n",
            "epoch 81\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.063869 Tokens per Sec: 2728.929443\n",
            "Epoch Step: 51 Loss: 0.058339 Tokens per Sec: 5990.151855\n",
            "\n",
            "epoch 82\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.052255 Tokens per Sec: 2730.333984\n",
            "Epoch Step: 51 Loss: 0.058957 Tokens per Sec: 6080.777832\n",
            "\n",
            "epoch 83\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.050301 Tokens per Sec: 1950.522949\n",
            "Epoch Step: 51 Loss: 0.056883 Tokens per Sec: 6300.502441\n",
            "\n",
            "epoch 84\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.046787 Tokens per Sec: 2377.068848\n",
            "Epoch Step: 51 Loss: 0.047454 Tokens per Sec: 6224.387695\n",
            "\n",
            "epoch 85\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.053391 Tokens per Sec: 2711.393555\n",
            "Epoch Step: 51 Loss: 0.057919 Tokens per Sec: 5987.295898\n",
            "\n",
            "epoch 86\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.059721 Tokens per Sec: 2787.712646\n",
            "Epoch Step: 51 Loss: 0.055859 Tokens per Sec: 6061.026855\n",
            "\n",
            "epoch 87\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.050695 Tokens per Sec: 2083.552002\n",
            "Epoch Step: 51 Loss: 0.059252 Tokens per Sec: 6065.452637\n",
            "\n",
            "epoch 88\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.049864 Tokens per Sec: 2059.355225\n",
            "Epoch Step: 51 Loss: 0.056504 Tokens per Sec: 6295.719727\n",
            "\n",
            "epoch 89\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.062509 Tokens per Sec: 2564.454590\n",
            "Epoch Step: 51 Loss: 0.049282 Tokens per Sec: 6076.325684\n",
            "\n",
            "epoch 90\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.063230 Tokens per Sec: 2657.813477\n",
            "Epoch Step: 51 Loss: 0.054439 Tokens per Sec: 6097.848145\n",
            "\n",
            "epoch 91\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.049856 Tokens per Sec: 2027.211914\n",
            "Epoch Step: 51 Loss: 0.051024 Tokens per Sec: 6247.480469\n",
            "\n",
            "epoch 92\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.056597 Tokens per Sec: 2251.771973\n",
            "Epoch Step: 51 Loss: 0.058172 Tokens per Sec: 6345.377930\n",
            "\n",
            "epoch 93\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.039332 Tokens per Sec: 2760.732666\n",
            "Epoch Step: 51 Loss: 0.047351 Tokens per Sec: 5961.929688\n",
            "\n",
            "epoch 94\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.061056 Tokens per Sec: 2834.350830\n",
            "Epoch Step: 51 Loss: 0.045910 Tokens per Sec: 6049.859375\n",
            "\n",
            "epoch 95\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.042242 Tokens per Sec: 1863.444214\n",
            "Epoch Step: 51 Loss: 0.048446 Tokens per Sec: 6278.867676\n",
            "\n",
            "epoch 96\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.047033 Tokens per Sec: 2519.566895\n",
            "Epoch Step: 51 Loss: 0.053151 Tokens per Sec: 6190.800293\n",
            "\n",
            "epoch 97\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041232 Tokens per Sec: 2810.683105\n",
            "Epoch Step: 51 Loss: 0.043803 Tokens per Sec: 6139.473145\n",
            "\n",
            "epoch 98\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.070845 Tokens per Sec: 2702.658447\n",
            "Epoch Step: 51 Loss: 0.047785 Tokens per Sec: 6107.458984\n",
            "\n",
            "epoch 99\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.064076 Tokens per Sec: 2905.072266\n",
            "Epoch Step: 51 Loss: 0.041889 Tokens per Sec: 6011.699219\n",
            "\n",
            "epoch 100\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.036141 Tokens per Sec: 2224.991455\n",
            "Epoch Step: 51 Loss: 0.050442 Tokens per Sec: 6326.691406\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.018981 Tokens per Sec: 2063.764648\n",
            "valid loss: 0.0176052525639534\n",
            "\n",
            "epoch 101\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.048684 Tokens per Sec: 2826.210938\n",
            "Epoch Step: 51 Loss: 0.048799 Tokens per Sec: 6005.751953\n",
            "\n",
            "epoch 102\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.050672 Tokens per Sec: 2454.282715\n",
            "Epoch Step: 51 Loss: 0.046769 Tokens per Sec: 6185.383301\n",
            "\n",
            "epoch 103\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.048136 Tokens per Sec: 1920.459839\n",
            "Epoch Step: 51 Loss: 0.061301 Tokens per Sec: 6474.569824\n",
            "\n",
            "epoch 104\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.056471 Tokens per Sec: 2299.945557\n",
            "Epoch Step: 51 Loss: 0.042958 Tokens per Sec: 6358.446289\n",
            "\n",
            "epoch 105\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.046451 Tokens per Sec: 2829.244385\n",
            "Epoch Step: 51 Loss: 0.046884 Tokens per Sec: 6133.162598\n",
            "\n",
            "epoch 106\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041412 Tokens per Sec: 2753.991455\n",
            "Epoch Step: 51 Loss: 0.048964 Tokens per Sec: 6170.564941\n",
            "\n",
            "epoch 107\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.045474 Tokens per Sec: 2581.370361\n",
            "Epoch Step: 51 Loss: 0.048795 Tokens per Sec: 6138.216797\n",
            "\n",
            "epoch 108\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.043291 Tokens per Sec: 1888.272461\n",
            "Epoch Step: 51 Loss: 0.050806 Tokens per Sec: 6493.161133\n",
            "\n",
            "epoch 109\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.058980 Tokens per Sec: 2433.105225\n",
            "Epoch Step: 51 Loss: 0.045320 Tokens per Sec: 6450.060547\n",
            "\n",
            "epoch 110\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.043586 Tokens per Sec: 2769.260498\n",
            "Epoch Step: 51 Loss: 0.041204 Tokens per Sec: 6413.229980\n",
            "\n",
            "epoch 111\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.030810 Tokens per Sec: 2922.616455\n",
            "Epoch Step: 51 Loss: 0.034634 Tokens per Sec: 6291.319336\n",
            "\n",
            "epoch 112\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.051801 Tokens per Sec: 2863.702637\n",
            "Epoch Step: 51 Loss: 0.041948 Tokens per Sec: 6078.738281\n",
            "\n",
            "epoch 113\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.040531 Tokens per Sec: 2678.159180\n",
            "Epoch Step: 51 Loss: 0.046884 Tokens per Sec: 6192.760742\n",
            "\n",
            "epoch 114\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.036085 Tokens per Sec: 2128.692383\n",
            "Epoch Step: 51 Loss: 0.038931 Tokens per Sec: 6421.480957\n",
            "\n",
            "epoch 115\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041050 Tokens per Sec: 1825.927002\n",
            "Epoch Step: 51 Loss: 0.043672 Tokens per Sec: 6530.814941\n",
            "\n",
            "epoch 116\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.049949 Tokens per Sec: 2529.828125\n",
            "Epoch Step: 51 Loss: 0.049666 Tokens per Sec: 6483.939453\n",
            "\n",
            "epoch 117\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.044419 Tokens per Sec: 2888.337402\n",
            "Epoch Step: 51 Loss: 0.039057 Tokens per Sec: 6308.678223\n",
            "\n",
            "epoch 118\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.036210 Tokens per Sec: 2792.446289\n",
            "Epoch Step: 51 Loss: 0.051549 Tokens per Sec: 6162.609863\n",
            "\n",
            "epoch 119\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.039516 Tokens per Sec: 2709.554443\n",
            "Epoch Step: 51 Loss: 0.050040 Tokens per Sec: 6201.489746\n",
            "\n",
            "epoch 120\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.044611 Tokens per Sec: 2494.935303\n",
            "Epoch Step: 51 Loss: 0.045890 Tokens per Sec: 6235.907227\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.017274 Tokens per Sec: 2139.138184\n",
            "valid loss: 0.016911914572119713\n",
            "\n",
            "epoch 121\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034926 Tokens per Sec: 2591.994629\n",
            "Epoch Step: 51 Loss: 0.041974 Tokens per Sec: 6126.827148\n",
            "\n",
            "epoch 122\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.044455 Tokens per Sec: 2629.426025\n",
            "Epoch Step: 51 Loss: 0.041908 Tokens per Sec: 6178.173340\n",
            "\n",
            "epoch 123\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.044294 Tokens per Sec: 2098.572021\n",
            "Epoch Step: 51 Loss: 0.041666 Tokens per Sec: 6322.183594\n",
            "\n",
            "epoch 124\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041475 Tokens per Sec: 1895.058594\n",
            "Epoch Step: 51 Loss: 0.032510 Tokens per Sec: 6380.308594\n",
            "\n",
            "epoch 125\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041137 Tokens per Sec: 2957.047607\n",
            "Epoch Step: 51 Loss: 0.039947 Tokens per Sec: 6315.708984\n",
            "\n",
            "epoch 126\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.039639 Tokens per Sec: 2807.865967\n",
            "Epoch Step: 51 Loss: 0.040330 Tokens per Sec: 6150.312500\n",
            "\n",
            "epoch 127\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.056658 Tokens per Sec: 2842.379395\n",
            "Epoch Step: 51 Loss: 0.041612 Tokens per Sec: 6144.606934\n",
            "\n",
            "epoch 128\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.040005 Tokens per Sec: 2462.159912\n",
            "Epoch Step: 51 Loss: 0.039634 Tokens per Sec: 6121.284668\n",
            "\n",
            "epoch 129\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.035874 Tokens per Sec: 1875.844849\n",
            "Epoch Step: 51 Loss: 0.039811 Tokens per Sec: 6481.389648\n",
            "\n",
            "epoch 130\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.043020 Tokens per Sec: 2032.173584\n",
            "Epoch Step: 51 Loss: 0.041299 Tokens per Sec: 6426.489746\n",
            "\n",
            "epoch 131\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.043194 Tokens per Sec: 2717.000732\n",
            "Epoch Step: 51 Loss: 0.035048 Tokens per Sec: 6469.311035\n",
            "\n",
            "epoch 132\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.038295 Tokens per Sec: 2742.712891\n",
            "Epoch Step: 51 Loss: 0.040444 Tokens per Sec: 6221.373535\n",
            "\n",
            "epoch 133\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.040949 Tokens per Sec: 2891.849854\n",
            "Epoch Step: 51 Loss: 0.049543 Tokens per Sec: 5986.851562\n",
            "\n",
            "epoch 134\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.038203 Tokens per Sec: 2701.682373\n",
            "Epoch Step: 51 Loss: 0.040325 Tokens per Sec: 6009.504883\n",
            "\n",
            "epoch 135\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.032365 Tokens per Sec: 1823.619995\n",
            "Epoch Step: 51 Loss: 0.040056 Tokens per Sec: 6376.710449\n",
            "\n",
            "epoch 136\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041365 Tokens per Sec: 2880.043213\n",
            "Epoch Step: 51 Loss: 0.040178 Tokens per Sec: 6132.739258\n",
            "\n",
            "epoch 137\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.036254 Tokens per Sec: 2732.392578\n",
            "Epoch Step: 51 Loss: 0.037822 Tokens per Sec: 6095.273926\n",
            "\n",
            "epoch 138\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.045259 Tokens per Sec: 2458.053711\n",
            "Epoch Step: 51 Loss: 0.032989 Tokens per Sec: 6116.782227\n",
            "\n",
            "epoch 139\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.036472 Tokens per Sec: 1938.270996\n",
            "Epoch Step: 51 Loss: 0.034126 Tokens per Sec: 6527.236816\n",
            "\n",
            "epoch 140\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.032830 Tokens per Sec: 2586.172119\n",
            "Epoch Step: 51 Loss: 0.041170 Tokens per Sec: 6430.269531\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.017073 Tokens per Sec: 2772.723877\n",
            "valid loss: 0.017145168036222458\n",
            "\n",
            "epoch 141\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.039406 Tokens per Sec: 1890.321777\n",
            "Epoch Step: 51 Loss: 0.041336 Tokens per Sec: 6459.580566\n",
            "\n",
            "epoch 142\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.035004 Tokens per Sec: 2030.430420\n",
            "Epoch Step: 51 Loss: 0.040748 Tokens per Sec: 6453.351562\n",
            "\n",
            "epoch 143\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037887 Tokens per Sec: 2788.058105\n",
            "Epoch Step: 51 Loss: 0.033023 Tokens per Sec: 6514.532715\n",
            "\n",
            "epoch 144\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037421 Tokens per Sec: 2885.558105\n",
            "Epoch Step: 51 Loss: 0.037842 Tokens per Sec: 6296.521973\n",
            "\n",
            "epoch 145\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037992 Tokens per Sec: 3006.202881\n",
            "Epoch Step: 51 Loss: 0.043047 Tokens per Sec: 6171.791992\n",
            "\n",
            "epoch 146\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.035905 Tokens per Sec: 2709.192871\n",
            "Epoch Step: 51 Loss: 0.035824 Tokens per Sec: 6169.085449\n",
            "\n",
            "epoch 147\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037969 Tokens per Sec: 2782.455566\n",
            "Epoch Step: 51 Loss: 0.036446 Tokens per Sec: 6215.204590\n",
            "\n",
            "epoch 148\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034419 Tokens per Sec: 2300.335693\n",
            "Epoch Step: 51 Loss: 0.034164 Tokens per Sec: 6310.124023\n",
            "\n",
            "epoch 149\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034985 Tokens per Sec: 1872.012329\n",
            "Epoch Step: 51 Loss: 0.042314 Tokens per Sec: 6552.696777\n",
            "\n",
            "epoch 150\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041109 Tokens per Sec: 2332.614258\n",
            "Epoch Step: 51 Loss: 0.034741 Tokens per Sec: 6599.802246\n",
            "\n",
            "epoch 151\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.031298 Tokens per Sec: 2833.546143\n",
            "Epoch Step: 51 Loss: 0.034182 Tokens per Sec: 6520.379395\n",
            "\n",
            "epoch 152\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037697 Tokens per Sec: 2714.144775\n",
            "Epoch Step: 51 Loss: 0.035558 Tokens per Sec: 6364.307129\n",
            "\n",
            "epoch 153\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034991 Tokens per Sec: 2910.635010\n",
            "Epoch Step: 51 Loss: 0.035676 Tokens per Sec: 6224.853027\n",
            "\n",
            "epoch 154\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041523 Tokens per Sec: 2729.845947\n",
            "Epoch Step: 51 Loss: 0.028736 Tokens per Sec: 6143.988770\n",
            "\n",
            "epoch 155\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034135 Tokens per Sec: 2669.665039\n",
            "Epoch Step: 51 Loss: 0.035405 Tokens per Sec: 6239.357422\n",
            "\n",
            "epoch 156\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.041953 Tokens per Sec: 1856.010132\n",
            "Epoch Step: 51 Loss: 0.035318 Tokens per Sec: 6416.021973\n",
            "\n",
            "epoch 157\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.035206 Tokens per Sec: 2471.579834\n",
            "Epoch Step: 51 Loss: 0.033527 Tokens per Sec: 6452.588867\n",
            "\n",
            "epoch 158\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.035990 Tokens per Sec: 2728.415283\n",
            "Epoch Step: 51 Loss: 0.035596 Tokens per Sec: 6430.922363\n",
            "\n",
            "epoch 159\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.030429 Tokens per Sec: 2835.742676\n",
            "Epoch Step: 51 Loss: 0.035603 Tokens per Sec: 6138.494629\n",
            "\n",
            "epoch 160\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.031944 Tokens per Sec: 2748.262207\n",
            "Epoch Step: 51 Loss: 0.035972 Tokens per Sec: 6200.716797\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.016316 Tokens per Sec: 2760.586670\n",
            "valid loss: 0.014538068324327469\n",
            "\n",
            "epoch 161\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037266 Tokens per Sec: 2700.289062\n",
            "Epoch Step: 51 Loss: 0.038660 Tokens per Sec: 6226.902832\n",
            "\n",
            "epoch 162\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.042315 Tokens per Sec: 2833.623535\n",
            "Epoch Step: 51 Loss: 0.029718 Tokens per Sec: 6207.920898\n",
            "\n",
            "epoch 163\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.023253 Tokens per Sec: 2658.240479\n",
            "Epoch Step: 51 Loss: 0.026081 Tokens per Sec: 6079.574219\n",
            "\n",
            "epoch 164\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.036418 Tokens per Sec: 2570.452148\n",
            "Epoch Step: 51 Loss: 0.038563 Tokens per Sec: 6071.695801\n",
            "\n",
            "epoch 165\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.032831 Tokens per Sec: 1823.237793\n",
            "Epoch Step: 51 Loss: 0.033253 Tokens per Sec: 6451.904297\n",
            "\n",
            "epoch 166\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.040108 Tokens per Sec: 2597.093994\n",
            "Epoch Step: 51 Loss: 0.033412 Tokens per Sec: 6460.875488\n",
            "\n",
            "epoch 167\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.027894 Tokens per Sec: 2873.399414\n",
            "Epoch Step: 51 Loss: 0.025064 Tokens per Sec: 6242.914551\n",
            "\n",
            "epoch 168\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.040001 Tokens per Sec: 2860.558105\n",
            "Epoch Step: 51 Loss: 0.026686 Tokens per Sec: 6122.941895\n",
            "\n",
            "epoch 169\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.032443 Tokens per Sec: 2710.450439\n",
            "Epoch Step: 51 Loss: 0.029856 Tokens per Sec: 6139.241699\n",
            "\n",
            "epoch 170\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034520 Tokens per Sec: 2761.564941\n",
            "Epoch Step: 51 Loss: 0.031402 Tokens per Sec: 6237.217285\n",
            "\n",
            "epoch 171\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.030860 Tokens per Sec: 2114.625000\n",
            "Epoch Step: 51 Loss: 0.027872 Tokens per Sec: 6370.936035\n",
            "\n",
            "epoch 172\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.033166 Tokens per Sec: 2250.166992\n",
            "Epoch Step: 51 Loss: 0.033329 Tokens per Sec: 6448.029785\n",
            "\n",
            "epoch 173\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.038547 Tokens per Sec: 2866.538330\n",
            "Epoch Step: 51 Loss: 0.029641 Tokens per Sec: 6089.246582\n",
            "\n",
            "epoch 174\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.029019 Tokens per Sec: 2858.640137\n",
            "Epoch Step: 51 Loss: 0.036844 Tokens per Sec: 6148.791992\n",
            "\n",
            "epoch 175\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.030012 Tokens per Sec: 2945.623047\n",
            "Epoch Step: 51 Loss: 0.027445 Tokens per Sec: 6184.366211\n",
            "\n",
            "epoch 176\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037239 Tokens per Sec: 2332.508789\n",
            "Epoch Step: 51 Loss: 0.029280 Tokens per Sec: 6346.499023\n",
            "\n",
            "epoch 177\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.024850 Tokens per Sec: 1968.357666\n",
            "Epoch Step: 51 Loss: 0.025154 Tokens per Sec: 6474.500977\n",
            "\n",
            "epoch 178\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.038009 Tokens per Sec: 2021.852539\n",
            "Epoch Step: 51 Loss: 0.027360 Tokens per Sec: 6490.878906\n",
            "\n",
            "epoch 179\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037884 Tokens per Sec: 2866.506592\n",
            "Epoch Step: 51 Loss: 0.034574 Tokens per Sec: 6303.313477\n",
            "\n",
            "epoch 180\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.029997 Tokens per Sec: 2766.425049\n",
            "Epoch Step: 51 Loss: 0.027014 Tokens per Sec: 6161.024902\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.016133 Tokens per Sec: 2744.861816\n",
            "valid loss: 0.015298847109079361\n",
            "\n",
            "epoch 181\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.039464 Tokens per Sec: 2745.677979\n",
            "Epoch Step: 51 Loss: 0.027343 Tokens per Sec: 6396.125977\n",
            "\n",
            "epoch 182\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.028172 Tokens per Sec: 2879.254150\n",
            "Epoch Step: 51 Loss: 0.032166 Tokens per Sec: 6099.946289\n",
            "\n",
            "epoch 183\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.030027 Tokens per Sec: 2798.412109\n",
            "Epoch Step: 51 Loss: 0.034479 Tokens per Sec: 6164.142578\n",
            "\n",
            "epoch 184\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.028682 Tokens per Sec: 2939.839600\n",
            "Epoch Step: 51 Loss: 0.029173 Tokens per Sec: 6094.999512\n",
            "\n",
            "epoch 185\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.031764 Tokens per Sec: 1983.531006\n",
            "Epoch Step: 51 Loss: 0.031393 Tokens per Sec: 6409.972656\n",
            "\n",
            "epoch 186\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034093 Tokens per Sec: 2133.224121\n",
            "Epoch Step: 51 Loss: 0.032866 Tokens per Sec: 6448.818848\n",
            "\n",
            "epoch 187\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.025665 Tokens per Sec: 2635.736084\n",
            "Epoch Step: 51 Loss: 0.028339 Tokens per Sec: 6132.391113\n",
            "\n",
            "epoch 188\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.028808 Tokens per Sec: 2672.496094\n",
            "Epoch Step: 51 Loss: 0.030866 Tokens per Sec: 6170.213867\n",
            "\n",
            "epoch 189\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.030100 Tokens per Sec: 2859.912842\n",
            "Epoch Step: 51 Loss: 0.035551 Tokens per Sec: 6142.366699\n",
            "\n",
            "epoch 190\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.028915 Tokens per Sec: 2357.676025\n",
            "Epoch Step: 51 Loss: 0.033654 Tokens per Sec: 6231.165527\n",
            "\n",
            "epoch 191\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.035072 Tokens per Sec: 1794.207764\n",
            "Epoch Step: 51 Loss: 0.026979 Tokens per Sec: 6484.615723\n",
            "\n",
            "epoch 192\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.026549 Tokens per Sec: 2374.958252\n",
            "Epoch Step: 51 Loss: 0.030142 Tokens per Sec: 6544.103027\n",
            "\n",
            "epoch 193\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034801 Tokens per Sec: 2795.058838\n",
            "Epoch Step: 51 Loss: 0.036529 Tokens per Sec: 6457.860352\n",
            "\n",
            "epoch 194\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.026908 Tokens per Sec: 2878.525391\n",
            "Epoch Step: 51 Loss: 0.038552 Tokens per Sec: 6306.454590\n",
            "\n",
            "epoch 195\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.038718 Tokens per Sec: 2807.493896\n",
            "Epoch Step: 51 Loss: 0.036485 Tokens per Sec: 6099.752930\n",
            "\n",
            "epoch 196\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.031729 Tokens per Sec: 2879.952393\n",
            "Epoch Step: 51 Loss: 0.028795 Tokens per Sec: 6202.895996\n",
            "\n",
            "epoch 197\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.029030 Tokens per Sec: 2331.381836\n",
            "Epoch Step: 51 Loss: 0.033811 Tokens per Sec: 6233.401855\n",
            "\n",
            "epoch 198\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037476 Tokens per Sec: 1978.687012\n",
            "Epoch Step: 51 Loss: 0.030312 Tokens per Sec: 6516.182617\n",
            "\n",
            "epoch 199\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.030161 Tokens per Sec: 2681.325195\n",
            "Epoch Step: 51 Loss: 0.024744 Tokens per Sec: 6106.164551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def acc_token(cur, pred):\n",
        "#   #cur = cur.numpy()\n",
        "#   pred = pred.numpy()\n",
        "#   if len(cur) != len(pred):\n",
        "#     while len(cur) != len(pred):\n",
        "#       pred = np.append(pred, 0)\n",
        "#     return (cur == pred).sum() / len(cur) * 100\n",
        "  \n",
        "#   else:\n",
        "#     return (cur == pred).sum() / len(cur) * 100"
      ],
      "metadata": {
        "id": "Z3Cb6mnLkY7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "1weHt51L5PXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ocr_model.eval()\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"greedy decode trainset\")\n",
        "total_img_num = 0\n",
        "total_correct_num = 0\n",
        "start = time.time()\n",
        "\n",
        "for batch_idx, batch in enumerate(train_loader):\n",
        "  img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "  img_input = img_input.to(device)                        \n",
        "  encode_mask = encode_mask.to(device)                                \n",
        "\n",
        "  bs = img_input.shape[0]\n",
        "  for i in range(bs):\n",
        "      cur_img_input = img_input[i].unsqueeze(0)\n",
        "      cur_encode_mask = encode_mask[i].unsqueeze(0)\n",
        "      cur_decode_out = decode_out[i]\n",
        "      #gredy_decode max_len = sequence_len\n",
        "      pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=11, start_symbol=1, end_symbol=2)\n",
        "      pred_result = pred_result.cpu()\n",
        "      #acc_token = acc_token(cur_decode_out, pred_result)\n",
        "\n",
        "      is_correct = check_correct(pred_result, cur_decode_out)\n",
        "      total_correct_num += is_correct\n",
        "      total_img_num += 1\n",
        "      # if not is_correct:\n",
        "      #     print(\"----\")\n",
        "      #     print(\"имеющаяся последовательность\", cur_decode_out)\n",
        "      #     print(\"предсказанная последовательность\", pred_result)\n",
        "end = time.time()\n",
        "\n",
        "total_correct_rate = total_correct_num / total_img_num * 100\n",
        "print(f\"total correct rate of trainset: {total_correct_rate}%\")\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end-start) * 10**3, \"ms\")"
      ],
      "metadata": {
        "id": "QOM9sT_dW3Ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e6ce91-476c-4829-fadb-42df0d7d15f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------\n",
            "greedy decode trainset\n",
            "total correct rate of trainset: 98.17747357336681%\n",
            "The time of execution of above program is : 3178194.90814209 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"greedy decode validset\")\n",
        "total_img_num = 0\n",
        "total_correct_num = 0\n",
        "start = time.time()\n",
        "for batch_idx, batch in enumerate(valid_loader):\n",
        "  img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "  img_input = img_input.to(device)                        \n",
        "  encode_mask = encode_mask.to(device)                                \n",
        "\n",
        "  bs = img_input.shape[0]\n",
        "  for i in range(bs):\n",
        "      cur_img_input = img_input[i].unsqueeze(0)\n",
        "      cur_encode_mask = encode_mask[i].unsqueeze(0)\n",
        "      cur_decode_out = decode_out[i]\n",
        "      \n",
        "      pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=11, start_symbol=1, end_symbol=2)\n",
        "      pred_result = pred_result.cpu()\n",
        "\n",
        "      is_correct = check_correct(pred_result, cur_decode_out)\n",
        "      total_correct_num += is_correct\n",
        "      total_img_num += 1\n",
        "      # if not is_correct:\n",
        "      #     print(\"имеющаяся последовательность\", cur_decode_out)\n",
        "      #     print(\"предсказанная последовательность\", pred_result)\n",
        "end = time.time()\n",
        " \n",
        "total_correct_rate = total_correct_num / total_img_num * 100\n",
        "print(f\"total correct rate of validset: {total_correct_rate}%\")\n",
        "# print the difference between start\n",
        "# and end time in milli. secs\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end-start) * 10**3, \"ms\")"
      ],
      "metadata": {
        "id": "jRZun91XW7p1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb03b85e-5682-4fdb-899d-4cee7aa3f04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------\n",
            "greedy decode validset\n",
            "total correct rate of validset: 95.97384017984876%\n",
            "The time of execution of above program is : 319013.70668411255 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"greedy decode testset\")\n",
        "total_img_num = 0\n",
        "total_correct_num = 0\n",
        "start = time.time()\n",
        "for batch_idx, batch in enumerate(test_loader):\n",
        "  img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "  img_input = img_input.to(device)                        \n",
        "  encode_mask = encode_mask.to(device)                                \n",
        "\n",
        "  bs = img_input.shape[0]\n",
        "  for i in range(bs):\n",
        "      cur_img_input = img_input[i].unsqueeze(0)\n",
        "      cur_encode_mask = encode_mask[i].unsqueeze(0)\n",
        "      cur_decode_out = decode_out[i]\n",
        "      \n",
        "      pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=11, start_symbol=1, end_symbol=2)\n",
        "      pred_result = pred_result.cpu()\n",
        "\n",
        "      is_correct = check_correct(pred_result, cur_decode_out)\n",
        "      total_correct_num += is_correct\n",
        "      total_img_num += 1\n",
        "      # if not is_correct:\n",
        "      #     print(\"имеющаяся последовательность\", cur_decode_out)\n",
        "      #     print(\"предсказанная последовательность\", pred_result)\n",
        "end = time.time()\n",
        "\n",
        "total_correct_rate = total_correct_num / total_img_num * 100\n",
        "print(f\"total correct rate of testset: {total_correct_rate}%\")\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end-start) * 10**3, \"ms\")"
      ],
      "metadata": {
        "id": "fB9B0RrckTYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b935c2ef-e0a2-499b-bd5a-fa2a59d5ef2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------\n",
            "greedy decode testset\n",
            "total correct rate of testset: 94.97363796133568%\n",
            "The time of execution of above program is : 185765.48719406128 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHZhuL_h76Oz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
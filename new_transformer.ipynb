{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammadsulton1/ML/blob/main/new_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "et6bwmOVHO2s"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "zipfile = '/content/drive/MyDrive/autoriaNumberplateOcrRu-2021-09-01.tar.gz'\n",
        "if zipfile.endswith(\"tar.gz\"):\n",
        "    tar = tarfile.open(zipfile, \"r:gz\")\n",
        "elif zipfile.endswith(\"tar\"):\n",
        "    tar = tarfile.open(zipfile, \"r:\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gLpk-DPuMuiT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ayzGKzsz7_BR"
      },
      "outputs": [],
      "source": [
        "def json_to_txt(path_name_to_save,path):\n",
        "  for name in os.listdir(path):\n",
        "    with open(os.path.join(path, name), 'r') as f:\n",
        "      data = json.loads(f.read())\n",
        "      img_name = data.get('name') + '.png'\n",
        "      description = data.get('description')\n",
        "      with open(path_name_to_save + \".txt\", \"a\") as text_file:\n",
        "        text_file.write('{},{}'.format(img_name, description + '\\n'))\n",
        "        text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mzUFVEMfNGz6"
      },
      "outputs": [],
      "source": [
        "json_to_txt('train_label','OCR_Licplate/train/ann')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T5ulZbKeNeiz"
      },
      "outputs": [],
      "source": [
        "json_to_txt('valid_label','OCR_Licplate/val/ann')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sg1dnljXciwA"
      },
      "outputs": [],
      "source": [
        "json_to_txt('test_label','OCR_Licplate/test/ann')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bkjNC1o-NVWU"
      },
      "outputs": [],
      "source": [
        "def statistics_label_cnt(lbl_path, lbl_cnt_map):\n",
        "    with open(lbl_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split(',')\n",
        "            img_name = items[0]\n",
        "            lbl_str = items[1].strip()\n",
        "            for lbl in lbl_str:\n",
        "                if lbl not in lbl_cnt_map.keys():\n",
        "                    lbl_cnt_map[lbl] = 1\n",
        "                else:\n",
        "                    lbl_cnt_map[lbl] += 1\n",
        "\n",
        "\n",
        "def statistics_max_len_label(lbl_path):\n",
        "    max_len = -1\n",
        "    with open(lbl_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split(',')\n",
        "            img_name = items[0]\n",
        "            lbl_str = items[1].strip()\n",
        "            lbl_len = len(lbl_str)\n",
        "            max_len = max_len if max_len > lbl_len else lbl_len\n",
        "    return max_len\n",
        "\n",
        "\n",
        "def load_lbl2id_map(lbl2id_map_path):\n",
        "    lbl2id_map = dict()\n",
        "    id2lbl_map = dict()\n",
        "    with open(lbl2id_map_path, 'r') as reader:\n",
        "        for line in reader:\n",
        "            items = line.rstrip().split(' ')\n",
        "            label = items[0]\n",
        "            cur_id = int(items[1])\n",
        "            lbl2id_map[label] = cur_id\n",
        "            id2lbl_map[cur_id] = label\n",
        "    return lbl2id_map, id2lbl_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DH886dAOGrb",
        "outputId": "6f55e316-097f-4e8c-95f7-d048ca108e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max len 9\n"
          ]
        }
      ],
      "source": [
        "base_data_dir = '/content/OCR_Licplate'\n",
        "\n",
        "train_img_dir = os.path.join(base_data_dir, 'train')\n",
        "valid_img_dir = os.path.join(base_data_dir, 'val')\n",
        "train_lbl_path = 'train_label.txt'\n",
        "valid_lbl_path = 'valid_label.txt'\n",
        "lbl2id_map_path = 'lbl2id_map.txt'\n",
        "\n",
        "train_max_label_len = statistics_max_len_label(train_lbl_path)\n",
        "valid_max_label_len = statistics_max_len_label(valid_lbl_path)\n",
        "max_label_len = max(train_max_label_len, valid_max_label_len)\n",
        "print(f\"max len {max_label_len}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PzSKhk7_QFZp"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Aeqm6Sc4Qj05"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
        "\n",
        "\n",
        "# class SimpleLossCompute:\n",
        "#     \"A simple loss compute and train function.\"\n",
        "#     def __init__(self, generator, criterion, opt=None):\n",
        "#         self.generator = generator\n",
        "#         self.criterion = criterion\n",
        "#         self.opt = opt\n",
        "        \n",
        "#     def __call__(self, x, y, norm):\n",
        "#         x = self.generator(x)\n",
        "#         x_ = x.contiguous().view(-1, x.size(-1))\n",
        "#         y_ = y.contiguous().view(-1)\n",
        "#         loss = self.criterion(x_, y_)\n",
        "#         loss /= norm\n",
        "#         loss.backward()\n",
        "#         if self.opt is not None:\n",
        "#             self.opt.step()\n",
        "#             self.opt.zero_grad()\n",
        "#         return loss.item() * norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTUvOyxI-D-k"
      },
      "source": [
        "#simplelosscompute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lFYxf342-DbU"
      },
      "outputs": [],
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        \"\"\"\n",
        "        norm: loss的归一化系数，用batch中所有有效token数即可\n",
        "        \"\"\"\n",
        "        x = self.generator(x)\n",
        "        x_ = x.contiguous().view(-1, x.size(-1))\n",
        "        y_ = y.contiguous().view(-1)\n",
        "        loss = self.criterion(x_, y_)\n",
        "        loss /= norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        #return loss.data[0] * norm  # TODO\n",
        "        return loss.item() * norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1qAJw5S2QFhw"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder\n",
        "    The encoder is composed of a stack of N=6 identical layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# We employ a residual connection around each of the two sub-layers, followed by layer normalization\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, feature_size, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(feature_size))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(feature_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        sublayer_out = sublayer(x)\n",
        "        sublayer_out = self.dropout(sublayer_out)\n",
        "        x_norm = x + self.norm(sublayer_out)\n",
        "        return x_norm\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        # attention sub layer\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        # feed forward sub layer\n",
        "        z = self.sublayer[1](x, self.feed_forward)\n",
        "        return z\n",
        "\n",
        "\n",
        "# Decoder\n",
        "# The decoder is also composed of a stack of N=6 identical layers.\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "\n",
        "# Attention\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "\n",
        "# Embeddings and Softmax\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedds = self.lut(x)\n",
        "        return embedds * math.sqrt(self.d_model)\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lA7e4iEEXvju"
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the warmup scheduler as a :class:`dict`.\n",
        "        It contains an entry for every variable in self.__dict__ which\n",
        "        is not the optimizer.\n",
        "        \"\"\"\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "    \n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the warmup scheduler's state.\n",
        "        Arguments:\n",
        "            state_dict (dict): warmup scheduler state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state_dict) \n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHD5dyPoRxtw"
      },
      "source": [
        "#OCR_BY_TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8E6HogORRs94"
      },
      "outputs": [],
      "source": [
        "class Data_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset_root_dir, lbl2id_map, sequence_len, phase='train', pad=0):\n",
        "        if phase == 'train':\n",
        "            self.img_dir = os.path.join(dataset_root_dir, 'train/img/')\n",
        "            self.lbl_path = 'train_label.txt'\n",
        "        elif phase == 'valid':\n",
        "            self.img_dir = os.path.join(dataset_root_dir, 'val/img/')\n",
        "            self.lbl_path = 'valid_label.txt'\n",
        "        else:\n",
        "            self.img_dir = os.path.join(dataset_root_dir, 'test/img/')\n",
        "            self.lbl_path = 'test_label.txt'\n",
        "\n",
        "        self.lbl2id_map = lbl2id_map\n",
        "        self.pad = pad\n",
        "        self.sequence_len = sequence_len\n",
        "\n",
        "        self.imgs_list = []\n",
        "        self.lbls_list = []\n",
        "\n",
        "        with open(self.lbl_path, 'r') as reader:\n",
        "            for line in reader:\n",
        "                items = line.rstrip().split(',')\n",
        "                img_name = items[0]\n",
        "                lbl_str = items[1]\n",
        "\n",
        "                self.imgs_list.append(img_name)\n",
        "                self.lbls_list.append(lbl_str)\n",
        "\n",
        "        # self.color_trans = transforms.ColorJitter(0.1, 0.1, 0.1)\n",
        "        self.trans_Normalize = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "            transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.imgs_list[index]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        lbl_str = self.lbls_list[index]\n",
        "\n",
        "        # load image\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        h_new = int(32)\n",
        "        w_new = int(160)\n",
        "        img_resize = img.resize((w_new, h_new), Image.BILINEAR)\n",
        "\n",
        "\n",
        "        img_input = self.trans_Normalize(img_resize)\n",
        "        # encode_mask = [1] * ratio + [0] * (self.max_ratio - ratio)\n",
        "        encode_mask = [1] * int(56) + [0] * int(24) #112 + 112 (66 + 14)\n",
        "        encode_mask = torch.tensor(encode_mask)\n",
        "        encode_mask = (encode_mask != 0).unsqueeze(0)\n",
        "\n",
        "        # ground truth label\n",
        "        gt = []\n",
        "        gt.append(1)\n",
        "        for lbl in lbl_str:\n",
        "            gt.append(self.lbl2id_map[lbl])\n",
        "        gt.append(2)\n",
        "        for i in range(len(lbl_str), self.sequence_len):\n",
        "            gt.append(0)\n",
        "        gt = gt[:self.sequence_len + 2]\n",
        "\n",
        "        # decoder\n",
        "        decode_in = gt[:-1]\n",
        "        decode_in = torch.tensor(decode_in)\n",
        "        # decoder\n",
        "        decode_out = gt[1:]\n",
        "        decode_out = torch.tensor(decode_out)\n",
        "        # decoder_mask\n",
        "        decode_mask = self.make_std_mask(decode_in, self.pad)\n",
        "        # tokens\n",
        "        ntokens = (decode_out != self.pad).data.sum()\n",
        "\n",
        "        return img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens\n",
        "\n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "\n",
        "        tgt_mask = (tgt != pad)\n",
        "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        tgt_mask = tgt_mask.squeeze(0)  # subsequent (1, N, N)\n",
        "        return tgt_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IZdP8xcplZ9G"
      },
      "outputs": [],
      "source": [
        "def see_plot(pict, color='gray', size=(4,4), title='None'):\n",
        "    plt.figure(figsize=size)\n",
        "    plt.imshow(pict, cmap=color)\n",
        "    plt.title(title)\n",
        "    #plt.grid()\n",
        "    #plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "def plot_samples_on_epoch(samples, nrow=4, chanels=3, size=(16,16), title='None'):\n",
        "    grid_img = torchvision.utils.make_grid(samples, nrow=nrow)\n",
        "    if chanels==1:\n",
        "         see_plot(grid_img.permute(1, 2, 0)*255, size=size, title=title)\n",
        "    else:\n",
        "         see_plot(grid_img.permute(1, 2, 0), size=size, title=title)\n",
        "    #see_plot(grid_img, size=size, title=title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "534Dz8cgSHZ_"
      },
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "class OCR_EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. \n",
        "    Base for this and many other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, decoder, src_embed, src_position, tgt_embed, generator):\n",
        "        super(OCR_EncoderDecoder, self).__init__()\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed    # input embedding module(input embedding + positional encode)\n",
        "        self.src_position = src_position\n",
        "        self.tgt_embed = tgt_embed    # ouput embedding module\n",
        "        self.generator = generator    # output generation module\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        memory = self.encode(src, src_mask)\n",
        "        res = self.decode(memory, src_mask, tgt, tgt_mask)\n",
        "        return res\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        # feature extract\n",
        "        src_embedds = self.src_embed(src)\n",
        "        src_embedds = torch.flatten(src_embedds,2,3)\n",
        "        src_embedds = src_embedds.permute(0, 2, 1)\n",
        "\n",
        "        # position encode\n",
        "        # src_embedds = self.src_position(src_embedds)\n",
        "        return src_embedds\n",
        "\n",
        "        # return self.encoder(src_embedds, src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        target_embedds = self.tgt_embed(tgt)\n",
        "        return self.decoder(target_embedds, memory, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "def make_ocr_model(tgt_vocab, N=3, d_model=128, d_ff=128*4, h=4, dropout=0.1):\n",
        "    c = copy.deepcopy\n",
        "\n",
        "    backbone = models.resnet18(pretrained=True)\n",
        "    backbone = nn.Sequential(*list(backbone.children())[:-4])\n",
        "\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "    model = OCR_EncoderDecoder(\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        backbone,\n",
        "        c(position),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    for child in model.children():\n",
        "        if child is backbone:\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "            continue\n",
        "        for p in child.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0Oy2vO4QRtHp"
      },
      "outputs": [],
      "source": [
        "def run_epoch(data_loader, model, loss_compute, device=None):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        #if device == \"cuda\":\n",
        "        #    batch.to_device(device) \n",
        "        img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "        img_input = img_input.to(device)                        \n",
        "        encode_mask = encode_mask.to(device)                                \n",
        "        decode_in = decode_in.to(device)                                \n",
        "        decode_out = decode_out.to(device)                    \n",
        "        decode_mask = decode_mask.to(device)\n",
        "        ntokens = torch.sum(ntokens).to(device)\n",
        "\n",
        "        out = model.forward(img_input, decode_in, encode_mask, decode_mask)\n",
        "\n",
        "        loss = loss_compute(out, decode_out, ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += ntokens\n",
        "        tokens += ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens\n",
        "\n",
        "\n",
        "# greedy decode\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol, end_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data).long()\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        next_word = torch.ones(1, 1).type_as(src.data).fill_(next_word).long()\n",
        "        ys = torch.cat([ys, next_word], dim=1)\n",
        "\n",
        "        next_word = int(next_word)\n",
        "        if next_word == end_symbol:\n",
        "            break\n",
        "        #ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    ys = ys[0, 1:]\n",
        "    return ys\n",
        "\n",
        "\n",
        "# def check_correct(pred, label):\n",
        "#     pred_len = pred.shape[0]\n",
        "#     label = label[:pred_len]\n",
        "#     is_correct = 1 if label.equal(pred) else 0\n",
        "#     return is_correct\n",
        "\n",
        "def check_correct(pred, label):\n",
        "  label_non_zero = label.nonzero().shape[0]\n",
        "  label = label[:label_non_zero]\n",
        "  is_correct = 1 if label.equal(pred) else 0\n",
        "  return is_correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nSawTR_PSceA"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "nrof_epochs = 200\n",
        "batch_size = 512\n",
        "model_save_path = 'ex1_ocr_model.pth'\n",
        "seq_len = 10\n",
        "\n",
        "lbl2id_map, id2lbl_map = load_lbl2id_map('/content/lbl2id_map.txt')\n",
        "\n",
        "sequence_len = max(train_max_label_len, valid_max_label_len)\n",
        "\n",
        "#dataloader\n",
        "train_dataset = Data_Dataset(base_data_dir, lbl2id_map, seq_len, 'train', pad=0) #sequence_len\n",
        "valid_dataset = Data_Dataset(base_data_dir, lbl2id_map, seq_len, 'valid', pad=0)\n",
        "test_dataset = Data_Dataset(base_data_dir, lbl2id_map, seq_len, 'test', pad=0)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=True,\n",
        "                                      num_workers=4)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      num_workers=4)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                      batch_size=batch_size,\n",
        "                                      shuffle=False,\n",
        "                                      num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "b5990f2e1df242c0b685503bb8520168",
            "7d683c3d3ac142f8b2df6334324b7c18",
            "b1caeebf1803444f9c92684ff3b6b3ef",
            "065cdb2accc14865a98f2e20aee96286",
            "9fe15609180f46b3a9ebeacce7551583",
            "74c5a054dff54fe08b67ddc8c0c8e376",
            "57138c11cef545a2adc50fd6cef16bdf",
            "7c1a02124231401bacaaef76dd2edb2a",
            "41800efcb7b8474885f9968657c9fcb7",
            "542e804d63c2465391aa86b4c4fb62d0",
            "2793b786813047d99d49fe5efacf1bd1"
          ]
        },
        "id": "e-lSo8IJWuIA",
        "outputId": "979ebcf7-58e3-494f-fefd-b4e39d26ab18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5990f2e1df242c0b685503bb8520168"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# build model\n",
        "# use transformer as ocr recognize model\n",
        "tgt_vocab = len(lbl2id_map.keys())\n",
        "d_model = 128\n",
        "ocr_model = make_ocr_model(tgt_vocab, N=3, d_model=d_model, d_ff=128*4, h=4, dropout=0.15)\n",
        "ocr_model.to(device)\n",
        "\n",
        "# train prepare\n",
        "criterion = LabelSmoothing(size=tgt_vocab, padding_idx=0, smoothing=0.0)\n",
        "\n",
        "# choose a optimizer\n",
        "#optimizer = torch.optim.Adam(ocr_model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
        "optimizer = torch.optim.Adam(ocr_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "#optimizer = torch.optim.SGD(ocr_model.parameters(), lr=0.001, momentum=0.9)\n",
        "model_opt = NoamOpt(d_model,200,optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY9jpE94qNmT",
        "outputId": "ba146e9a-2bd2-49f8-af4f-78c964b000e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 0\n",
            "train...\n",
            "Epoch Step: 1 Loss: 4.523272 Tokens per Sec: 770.037598\n",
            "Epoch Step: 51 Loss: 2.303601 Tokens per Sec: 7039.082031\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.864028 Tokens per Sec: 2389.475586\n",
            "valid loss: 0.8730087280273438\n",
            "\n",
            "epoch 1\n",
            "train...\n",
            "Epoch Step: 1 Loss: 1.908322 Tokens per Sec: 2830.612793\n",
            "Epoch Step: 51 Loss: 0.457384 Tokens per Sec: 6061.965820\n",
            "\n",
            "epoch 2\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.299798 Tokens per Sec: 3075.713623\n",
            "Epoch Step: 51 Loss: 0.223340 Tokens per Sec: 6441.885742\n",
            "\n",
            "epoch 3\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.215807 Tokens per Sec: 3070.316406\n",
            "Epoch Step: 51 Loss: 0.194136 Tokens per Sec: 6814.626465\n",
            "\n",
            "epoch 4\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.133249 Tokens per Sec: 3160.451172\n",
            "Epoch Step: 51 Loss: 0.135304 Tokens per Sec: 6799.019043\n",
            "\n",
            "epoch 5\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.129584 Tokens per Sec: 2889.802002\n",
            "Epoch Step: 51 Loss: 0.126175 Tokens per Sec: 6919.763672\n",
            "\n",
            "epoch 6\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.091068 Tokens per Sec: 2378.518799\n",
            "Epoch Step: 51 Loss: 0.113456 Tokens per Sec: 6841.621094\n",
            "\n",
            "epoch 7\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.100226 Tokens per Sec: 2122.315186\n",
            "Epoch Step: 51 Loss: 0.091433 Tokens per Sec: 6505.169434\n",
            "\n",
            "epoch 8\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.083223 Tokens per Sec: 1735.100952\n",
            "Epoch Step: 51 Loss: 0.080315 Tokens per Sec: 6525.963379\n",
            "\n",
            "epoch 9\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.066279 Tokens per Sec: 2882.535400\n",
            "Epoch Step: 51 Loss: 0.067038 Tokens per Sec: 6553.513672\n",
            "\n",
            "epoch 10\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.063924 Tokens per Sec: 3212.584229\n",
            "Epoch Step: 51 Loss: 0.072250 Tokens per Sec: 6477.581543\n",
            "\n",
            "epoch 11\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.057389 Tokens per Sec: 3034.253906\n",
            "Epoch Step: 51 Loss: 0.061031 Tokens per Sec: 6333.430664\n",
            "\n",
            "epoch 12\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.069063 Tokens per Sec: 2943.572754\n",
            "Epoch Step: 51 Loss: 0.063163 Tokens per Sec: 6754.163574\n",
            "\n",
            "epoch 13\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.054177 Tokens per Sec: 3033.951660\n",
            "Epoch Step: 51 Loss: 0.045870 Tokens per Sec: 6785.644043\n",
            "\n",
            "epoch 14\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.057654 Tokens per Sec: 2515.829834\n",
            "Epoch Step: 51 Loss: 0.049480 Tokens per Sec: 6810.012207\n",
            "\n",
            "epoch 15\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.042259 Tokens per Sec: 2738.224854\n",
            "Epoch Step: 51 Loss: 0.044685 Tokens per Sec: 6774.563965\n",
            "\n",
            "epoch 16\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.038116 Tokens per Sec: 2358.909424\n",
            "Epoch Step: 51 Loss: 0.040613 Tokens per Sec: 6824.097168\n",
            "\n",
            "epoch 17\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.044519 Tokens per Sec: 1946.860596\n",
            "Epoch Step: 51 Loss: 0.050186 Tokens per Sec: 6771.849609\n",
            "\n",
            "epoch 18\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.039417 Tokens per Sec: 1899.902344\n",
            "Epoch Step: 51 Loss: 0.042305 Tokens per Sec: 6452.331055\n",
            "\n",
            "epoch 19\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034452 Tokens per Sec: 2988.813477\n",
            "Epoch Step: 51 Loss: 0.032849 Tokens per Sec: 6454.544922\n",
            "\n",
            "epoch 20\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.026381 Tokens per Sec: 3136.916260\n",
            "Epoch Step: 51 Loss: 0.033901 Tokens per Sec: 6736.408203\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.039584 Tokens per Sec: 3020.861816\n",
            "valid loss: 0.03783508017659187\n",
            "\n",
            "epoch 21\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.035749 Tokens per Sec: 2615.276367\n",
            "Epoch Step: 51 Loss: 0.033137 Tokens per Sec: 6295.407227\n",
            "\n",
            "epoch 22\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037750 Tokens per Sec: 2246.545898\n",
            "Epoch Step: 51 Loss: 0.035976 Tokens per Sec: 6691.518066\n",
            "\n",
            "epoch 23\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.033663 Tokens per Sec: 3154.943359\n",
            "Epoch Step: 51 Loss: 0.029170 Tokens per Sec: 6465.825684\n",
            "\n",
            "epoch 24\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.033250 Tokens per Sec: 3042.481934\n",
            "Epoch Step: 51 Loss: 0.041311 Tokens per Sec: 6441.008789\n",
            "\n",
            "epoch 25\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.034493 Tokens per Sec: 3025.811523\n",
            "Epoch Step: 51 Loss: 0.041155 Tokens per Sec: 6650.539551\n",
            "\n",
            "epoch 26\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.037590 Tokens per Sec: 2955.560791\n",
            "Epoch Step: 51 Loss: 0.028182 Tokens per Sec: 6849.091797\n",
            "\n",
            "epoch 27\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.026798 Tokens per Sec: 2411.711182\n",
            "Epoch Step: 51 Loss: 0.027466 Tokens per Sec: 6867.333984\n",
            "\n",
            "epoch 28\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.027519 Tokens per Sec: 1849.687134\n",
            "Epoch Step: 51 Loss: 0.027042 Tokens per Sec: 6513.273926\n",
            "\n",
            "epoch 29\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.024875 Tokens per Sec: 2540.260986\n",
            "Epoch Step: 51 Loss: 0.031850 Tokens per Sec: 6711.507812\n",
            "\n",
            "epoch 30\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.023439 Tokens per Sec: 3115.066162\n",
            "Epoch Step: 51 Loss: 0.021345 Tokens per Sec: 6508.376465\n",
            "\n",
            "epoch 31\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.024905 Tokens per Sec: 3021.573975\n",
            "Epoch Step: 51 Loss: 0.035867 Tokens per Sec: 6606.124023\n",
            "\n",
            "epoch 32\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.027399 Tokens per Sec: 3143.741699\n",
            "Epoch Step: 51 Loss: 0.035726 Tokens per Sec: 6556.692871\n",
            "\n",
            "epoch 33\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.026606 Tokens per Sec: 3199.200195\n",
            "Epoch Step: 51 Loss: 0.024313 Tokens per Sec: 6817.812012\n",
            "\n",
            "epoch 34\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.025234 Tokens per Sec: 2914.121338\n",
            "Epoch Step: 51 Loss: 0.028742 Tokens per Sec: 6875.004883\n",
            "\n",
            "epoch 35\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.022105 Tokens per Sec: 1810.809448\n",
            "Epoch Step: 51 Loss: 0.025398 Tokens per Sec: 6833.225586\n",
            "\n",
            "epoch 36\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.024299 Tokens per Sec: 2280.913330\n",
            "Epoch Step: 51 Loss: 0.023176 Tokens per Sec: 6922.364746\n",
            "\n",
            "epoch 37\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.021713 Tokens per Sec: 2200.951904\n",
            "Epoch Step: 51 Loss: 0.025006 Tokens per Sec: 6814.292969\n",
            "\n",
            "epoch 38\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.016034 Tokens per Sec: 2888.672119\n",
            "Epoch Step: 51 Loss: 0.027192 Tokens per Sec: 6209.005859\n",
            "\n",
            "epoch 39\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.017048 Tokens per Sec: 2977.405762\n",
            "Epoch Step: 51 Loss: 0.022793 Tokens per Sec: 6617.595215\n",
            "\n",
            "epoch 40\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.022970 Tokens per Sec: 2978.189697\n",
            "Epoch Step: 51 Loss: 0.017651 Tokens per Sec: 6913.071777\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.036314 Tokens per Sec: 2941.755615\n",
            "valid loss: 0.0328698456287384\n",
            "\n",
            "epoch 41\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.018028 Tokens per Sec: 3052.897217\n",
            "Epoch Step: 51 Loss: 0.021024 Tokens per Sec: 6428.088379\n",
            "\n",
            "epoch 42\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.022356 Tokens per Sec: 3011.148682\n",
            "Epoch Step: 51 Loss: 0.019983 Tokens per Sec: 6556.404785\n",
            "\n",
            "epoch 43\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.025131 Tokens per Sec: 3035.485107\n",
            "Epoch Step: 51 Loss: 0.014665 Tokens per Sec: 6452.460449\n",
            "\n",
            "epoch 44\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.019326 Tokens per Sec: 3051.947021\n",
            "Epoch Step: 51 Loss: 0.020195 Tokens per Sec: 6644.194336\n",
            "\n",
            "epoch 45\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.014847 Tokens per Sec: 3010.526855\n",
            "Epoch Step: 51 Loss: 0.020187 Tokens per Sec: 6851.098633\n",
            "\n",
            "epoch 46\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.022903 Tokens per Sec: 2395.745361\n",
            "Epoch Step: 51 Loss: 0.025554 Tokens per Sec: 6839.750488\n",
            "\n",
            "epoch 47\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.021107 Tokens per Sec: 1920.419678\n",
            "Epoch Step: 51 Loss: 0.012446 Tokens per Sec: 6799.175293\n",
            "\n",
            "epoch 48\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.014625 Tokens per Sec: 2010.500732\n",
            "Epoch Step: 51 Loss: 0.021271 Tokens per Sec: 6786.954102\n",
            "\n",
            "epoch 49\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.021448 Tokens per Sec: 2342.302734\n",
            "Epoch Step: 51 Loss: 0.019288 Tokens per Sec: 6496.981445\n",
            "\n",
            "epoch 50\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.020970 Tokens per Sec: 2611.042725\n",
            "Epoch Step: 51 Loss: 0.018155 Tokens per Sec: 6522.962402\n",
            "\n",
            "epoch 51\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.016422 Tokens per Sec: 2940.836914\n",
            "Epoch Step: 51 Loss: 0.014338 Tokens per Sec: 6537.281738\n",
            "\n",
            "epoch 52\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.017067 Tokens per Sec: 3063.150879\n",
            "Epoch Step: 51 Loss: 0.017606 Tokens per Sec: 6414.268555\n",
            "\n",
            "epoch 53\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.023496 Tokens per Sec: 2950.962646\n",
            "Epoch Step: 51 Loss: 0.017951 Tokens per Sec: 6398.500000\n",
            "\n",
            "epoch 54\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009545 Tokens per Sec: 2945.175781\n",
            "Epoch Step: 51 Loss: 0.018282 Tokens per Sec: 6418.055176\n",
            "\n",
            "epoch 55\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.018263 Tokens per Sec: 3085.028564\n",
            "Epoch Step: 51 Loss: 0.025576 Tokens per Sec: 6554.601074\n",
            "\n",
            "epoch 56\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.022570 Tokens per Sec: 2892.735596\n",
            "Epoch Step: 51 Loss: 0.017941 Tokens per Sec: 6512.113281\n",
            "\n",
            "epoch 57\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.018483 Tokens per Sec: 3074.974609\n",
            "Epoch Step: 51 Loss: 0.016464 Tokens per Sec: 6451.743164\n",
            "\n",
            "epoch 58\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.018594 Tokens per Sec: 2919.067627\n",
            "Epoch Step: 51 Loss: 0.016541 Tokens per Sec: 6529.152344\n",
            "\n",
            "epoch 59\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.011956 Tokens per Sec: 2991.347900\n",
            "Epoch Step: 51 Loss: 0.016747 Tokens per Sec: 6549.932129\n",
            "\n",
            "epoch 60\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.017654 Tokens per Sec: 2919.727783\n",
            "Epoch Step: 51 Loss: 0.018349 Tokens per Sec: 6738.477539\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.039440 Tokens per Sec: 3094.976318\n",
            "valid loss: 0.030551116913557053\n",
            "\n",
            "epoch 61\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.012926 Tokens per Sec: 2993.407471\n",
            "Epoch Step: 51 Loss: 0.011993 Tokens per Sec: 6301.111328\n",
            "\n",
            "epoch 62\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010889 Tokens per Sec: 3030.011475\n",
            "Epoch Step: 51 Loss: 0.017045 Tokens per Sec: 6323.528320\n",
            "\n",
            "epoch 63\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.015189 Tokens per Sec: 3053.942383\n",
            "Epoch Step: 51 Loss: 0.014202 Tokens per Sec: 6301.626953\n",
            "\n",
            "epoch 64\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.011535 Tokens per Sec: 3093.225098\n",
            "Epoch Step: 51 Loss: 0.018534 Tokens per Sec: 6355.962891\n",
            "\n",
            "epoch 65\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.013096 Tokens per Sec: 3068.833008\n",
            "Epoch Step: 51 Loss: 0.016455 Tokens per Sec: 6314.278809\n",
            "\n",
            "epoch 66\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.012418 Tokens per Sec: 3006.957031\n",
            "Epoch Step: 51 Loss: 0.017663 Tokens per Sec: 6283.182617\n",
            "\n",
            "epoch 67\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.018623 Tokens per Sec: 2971.853516\n",
            "Epoch Step: 51 Loss: 0.017478 Tokens per Sec: 6301.494141\n",
            "\n",
            "epoch 68\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.017604 Tokens per Sec: 3015.584961\n",
            "Epoch Step: 51 Loss: 0.013868 Tokens per Sec: 6571.402832\n",
            "\n",
            "epoch 69\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.015098 Tokens per Sec: 3174.168213\n",
            "Epoch Step: 51 Loss: 0.018189 Tokens per Sec: 6791.625000\n",
            "\n",
            "epoch 70\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.012421 Tokens per Sec: 2984.145996\n",
            "Epoch Step: 51 Loss: 0.012631 Tokens per Sec: 6699.979492\n",
            "\n",
            "epoch 71\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010679 Tokens per Sec: 2665.130859\n",
            "Epoch Step: 51 Loss: 0.010139 Tokens per Sec: 6751.521973\n",
            "\n",
            "epoch 72\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.014095 Tokens per Sec: 2229.208740\n",
            "Epoch Step: 51 Loss: 0.015733 Tokens per Sec: 6833.583008\n",
            "\n",
            "epoch 73\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.012472 Tokens per Sec: 2260.422607\n",
            "Epoch Step: 51 Loss: 0.012324 Tokens per Sec: 6753.152344\n",
            "\n",
            "epoch 74\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.011904 Tokens per Sec: 3069.392334\n",
            "Epoch Step: 51 Loss: 0.007454 Tokens per Sec: 6533.826172\n",
            "\n",
            "epoch 75\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009609 Tokens per Sec: 2975.568115\n",
            "Epoch Step: 51 Loss: 0.012760 Tokens per Sec: 6534.266113\n",
            "\n",
            "epoch 76\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.012400 Tokens per Sec: 2854.567627\n",
            "Epoch Step: 51 Loss: 0.011814 Tokens per Sec: 6613.222168\n",
            "\n",
            "epoch 77\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.007970 Tokens per Sec: 3216.577393\n",
            "Epoch Step: 51 Loss: 0.018726 Tokens per Sec: 6880.016602\n",
            "\n",
            "epoch 78\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.018911 Tokens per Sec: 2563.125977\n",
            "Epoch Step: 51 Loss: 0.014822 Tokens per Sec: 6843.843262\n",
            "\n",
            "epoch 79\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.020386 Tokens per Sec: 2002.498169\n",
            "Epoch Step: 51 Loss: 0.010999 Tokens per Sec: 6884.740723\n",
            "\n",
            "epoch 80\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.012490 Tokens per Sec: 2511.400146\n",
            "Epoch Step: 51 Loss: 0.013350 Tokens per Sec: 6672.290527\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.036515 Tokens per Sec: 3166.945801\n",
            "valid loss: 0.02581743150949478\n",
            "\n",
            "epoch 81\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.007020 Tokens per Sec: 3091.546387\n",
            "Epoch Step: 51 Loss: 0.011666 Tokens per Sec: 6805.026855\n",
            "\n",
            "epoch 82\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.006500 Tokens per Sec: 2223.689453\n",
            "Epoch Step: 51 Loss: 0.015324 Tokens per Sec: 6833.890625\n",
            "\n",
            "epoch 83\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.012173 Tokens per Sec: 2053.289551\n",
            "Epoch Step: 51 Loss: 0.012574 Tokens per Sec: 6838.037109\n",
            "\n",
            "epoch 84\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.013676 Tokens per Sec: 3164.346924\n",
            "Epoch Step: 51 Loss: 0.014481 Tokens per Sec: 6454.065918\n",
            "\n",
            "epoch 85\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.005074 Tokens per Sec: 3109.439209\n",
            "Epoch Step: 51 Loss: 0.008160 Tokens per Sec: 6349.457031\n",
            "\n",
            "epoch 86\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009194 Tokens per Sec: 3066.946045\n",
            "Epoch Step: 51 Loss: 0.008175 Tokens per Sec: 6428.942383\n",
            "\n",
            "epoch 87\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009429 Tokens per Sec: 3098.641602\n",
            "Epoch Step: 51 Loss: 0.009701 Tokens per Sec: 6508.195312\n",
            "\n",
            "epoch 88\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.008585 Tokens per Sec: 3112.495117\n",
            "Epoch Step: 51 Loss: 0.012136 Tokens per Sec: 6680.396484\n",
            "\n",
            "epoch 89\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.013562 Tokens per Sec: 2939.601074\n",
            "Epoch Step: 51 Loss: 0.008894 Tokens per Sec: 6480.125488\n",
            "\n",
            "epoch 90\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.008469 Tokens per Sec: 3150.704102\n",
            "Epoch Step: 51 Loss: 0.010423 Tokens per Sec: 6799.406738\n",
            "\n",
            "epoch 91\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010317 Tokens per Sec: 2324.007324\n",
            "Epoch Step: 51 Loss: 0.013919 Tokens per Sec: 6779.500488\n",
            "\n",
            "epoch 92\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.011766 Tokens per Sec: 2058.615723\n",
            "Epoch Step: 51 Loss: 0.008829 Tokens per Sec: 6844.112793\n",
            "\n",
            "epoch 93\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009156 Tokens per Sec: 2115.039062\n",
            "Epoch Step: 51 Loss: 0.014624 Tokens per Sec: 6821.605957\n",
            "\n",
            "epoch 94\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009037 Tokens per Sec: 2925.706299\n",
            "Epoch Step: 51 Loss: 0.013973 Tokens per Sec: 6524.640137\n",
            "\n",
            "epoch 95\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.008330 Tokens per Sec: 3041.219727\n",
            "Epoch Step: 51 Loss: 0.009220 Tokens per Sec: 6436.162598\n",
            "\n",
            "epoch 96\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.008473 Tokens per Sec: 2945.352783\n",
            "Epoch Step: 51 Loss: 0.009559 Tokens per Sec: 6585.245605\n",
            "\n",
            "epoch 97\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010759 Tokens per Sec: 3088.130859\n",
            "Epoch Step: 51 Loss: 0.009599 Tokens per Sec: 6750.409180\n",
            "\n",
            "epoch 98\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010365 Tokens per Sec: 2943.995850\n",
            "Epoch Step: 51 Loss: 0.008915 Tokens per Sec: 6943.244629\n",
            "\n",
            "epoch 99\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010248 Tokens per Sec: 2828.392822\n",
            "Epoch Step: 51 Loss: 0.018985 Tokens per Sec: 6771.481445\n",
            "\n",
            "epoch 100\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009957 Tokens per Sec: 2322.504395\n",
            "Epoch Step: 51 Loss: 0.009934 Tokens per Sec: 6908.464844\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.029243 Tokens per Sec: 2062.035400\n",
            "valid loss: 0.022965207695961\n",
            "\n",
            "epoch 101\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.011797 Tokens per Sec: 3096.595947\n",
            "Epoch Step: 51 Loss: 0.014957 Tokens per Sec: 6440.651367\n",
            "\n",
            "epoch 102\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009148 Tokens per Sec: 3032.795898\n",
            "Epoch Step: 51 Loss: 0.009971 Tokens per Sec: 6874.524414\n",
            "\n",
            "epoch 103\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.006096 Tokens per Sec: 2585.781250\n",
            "Epoch Step: 51 Loss: 0.010202 Tokens per Sec: 6938.784180\n",
            "\n",
            "epoch 104\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010024 Tokens per Sec: 2072.440186\n",
            "Epoch Step: 51 Loss: 0.008786 Tokens per Sec: 6973.471191\n",
            "\n",
            "epoch 105\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.011923 Tokens per Sec: 2283.066650\n",
            "Epoch Step: 51 Loss: 0.006696 Tokens per Sec: 6722.753418\n",
            "\n",
            "epoch 106\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010046 Tokens per Sec: 3076.183105\n",
            "Epoch Step: 51 Loss: 0.013201 Tokens per Sec: 6493.018066\n",
            "\n",
            "epoch 107\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.006423 Tokens per Sec: 3100.763184\n",
            "Epoch Step: 51 Loss: 0.006309 Tokens per Sec: 6665.621094\n",
            "\n",
            "epoch 108\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009698 Tokens per Sec: 2994.116699\n",
            "Epoch Step: 51 Loss: 0.010112 Tokens per Sec: 6796.334961\n",
            "\n",
            "epoch 109\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.005811 Tokens per Sec: 2728.728760\n",
            "Epoch Step: 51 Loss: 0.009923 Tokens per Sec: 6757.371094\n",
            "\n",
            "epoch 110\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009066 Tokens per Sec: 2212.614258\n",
            "Epoch Step: 51 Loss: 0.011289 Tokens per Sec: 6913.145508\n",
            "\n",
            "epoch 111\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010696 Tokens per Sec: 2053.514648\n",
            "Epoch Step: 51 Loss: 0.007749 Tokens per Sec: 6708.999512\n",
            "\n",
            "epoch 112\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.011280 Tokens per Sec: 2593.971191\n",
            "Epoch Step: 51 Loss: 0.009560 Tokens per Sec: 6479.786133\n",
            "\n",
            "epoch 113\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.008948 Tokens per Sec: 3028.467773\n",
            "Epoch Step: 51 Loss: 0.007632 Tokens per Sec: 6390.125977\n",
            "\n",
            "epoch 114\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.013595 Tokens per Sec: 3056.026367\n",
            "Epoch Step: 51 Loss: 0.011699 Tokens per Sec: 6512.678223\n",
            "\n",
            "epoch 115\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.008190 Tokens per Sec: 3171.820801\n",
            "Epoch Step: 51 Loss: 0.007481 Tokens per Sec: 6664.371582\n",
            "\n",
            "epoch 116\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.005866 Tokens per Sec: 3133.092773\n",
            "Epoch Step: 51 Loss: 0.007493 Tokens per Sec: 6758.436035\n",
            "\n",
            "epoch 117\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010954 Tokens per Sec: 2604.136963\n",
            "Epoch Step: 51 Loss: 0.009219 Tokens per Sec: 6777.662109\n",
            "\n",
            "epoch 118\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.007494 Tokens per Sec: 1988.326172\n",
            "Epoch Step: 51 Loss: 0.009975 Tokens per Sec: 6754.330566\n",
            "\n",
            "epoch 119\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.006380 Tokens per Sec: 2468.529297\n",
            "Epoch Step: 51 Loss: 0.006840 Tokens per Sec: 6631.252930\n",
            "\n",
            "epoch 120\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.009642 Tokens per Sec: 2989.833008\n",
            "Epoch Step: 51 Loss: 0.007096 Tokens per Sec: 6569.564453\n",
            "valid...\n",
            "Epoch Step: 1 Loss: 0.028266 Tokens per Sec: 3127.118164\n",
            "valid loss: 0.02165377326309681\n",
            "\n",
            "epoch 121\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.006967 Tokens per Sec: 2697.700195\n",
            "Epoch Step: 51 Loss: 0.007070 Tokens per Sec: 6951.482422\n",
            "\n",
            "epoch 122\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.008722 Tokens per Sec: 1903.549683\n",
            "Epoch Step: 51 Loss: 0.008890 Tokens per Sec: 6841.180664\n",
            "\n",
            "epoch 123\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.005317 Tokens per Sec: 2730.596680\n",
            "Epoch Step: 51 Loss: 0.007990 Tokens per Sec: 6709.592285\n",
            "\n",
            "epoch 124\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010632 Tokens per Sec: 3174.591309\n",
            "Epoch Step: 51 Loss: 0.008221 Tokens per Sec: 6544.932129\n",
            "\n",
            "epoch 125\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.006512 Tokens per Sec: 3173.562012\n",
            "Epoch Step: 51 Loss: 0.013674 Tokens per Sec: 6871.496094\n",
            "\n",
            "epoch 126\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.005484 Tokens per Sec: 3032.780762\n",
            "Epoch Step: 51 Loss: 0.007001 Tokens per Sec: 6964.058594\n",
            "\n",
            "epoch 127\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.010753 Tokens per Sec: 2082.027588\n",
            "Epoch Step: 51 Loss: 0.008390 Tokens per Sec: 6930.312988\n",
            "\n",
            "epoch 128\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.005982 Tokens per Sec: 2787.562012\n",
            "Epoch Step: 51 Loss: 0.007667 Tokens per Sec: 6723.770020\n",
            "\n",
            "epoch 129\n",
            "train...\n",
            "Epoch Step: 1 Loss: 0.008229 Tokens per Sec: 3079.231201\n",
            "Epoch Step: 51 Loss: 0.006387 Tokens per Sec: 6477.733398\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(nrof_epochs):\n",
        "  print(f\"\\nepoch {epoch}\")\n",
        "\n",
        "  print(\"train...\")\n",
        "  ocr_model.train()\n",
        "  loss_compute = SimpleLossCompute(ocr_model.generator, criterion, model_opt)\n",
        "  #loss_compute = SimpleLossCompute(ocr_model.generator, criterion, optimizer)\n",
        "  train_mean_loss = run_epoch(train_loader, ocr_model, loss_compute, device)\n",
        "\n",
        "  if epoch % 20 == 0:\n",
        "    print(\"valid...\")\n",
        "    ocr_model.eval()\n",
        "    valid_loss_compute = SimpleLossCompute(ocr_model.generator, criterion, None)\n",
        "    valid_mean_loss = run_epoch(valid_loader, ocr_model, valid_loss_compute, device)\n",
        "    print(f\"valid loss: {valid_mean_loss}\")\n",
        "\n",
        "# save model\n",
        "torch.save(ocr_model.state_dict(), model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3Cb6mnLkY7k"
      },
      "outputs": [],
      "source": [
        "# def acc_token(cur, pred):\n",
        "#   #cur = cur.numpy()\n",
        "#   pred = pred.numpy()\n",
        "#   if len(cur) != len(pred):\n",
        "#     while len(cur) != len(pred):\n",
        "#       pred = np.append(pred, 0)\n",
        "#     return (cur == pred).sum() / len(cur) * 100\n",
        "  \n",
        "#   else:\n",
        "#     return (cur == pred).sum() / len(cur) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1weHt51L5PXo"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOM9sT_dW3Ot"
      },
      "outputs": [],
      "source": [
        "ocr_model.eval()\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"greedy decode trainset\")\n",
        "total_img_num = 0\n",
        "total_correct_num = 0\n",
        "start = time.time()\n",
        "\n",
        "for batch_idx, batch in enumerate(train_loader):\n",
        "  img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "  img_input = img_input.to(device)                        \n",
        "  encode_mask = encode_mask.to(device)                                \n",
        "\n",
        "  bs = img_input.shape[0]\n",
        "  for i in range(bs):\n",
        "      cur_img_input = img_input[i].unsqueeze(0)\n",
        "      cur_encode_mask = encode_mask[i].unsqueeze(0)\n",
        "      cur_decode_out = decode_out[i]\n",
        "      #gredy_decode max_len = sequence_len\n",
        "      pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=11, start_symbol=1, end_symbol=2)\n",
        "      pred_result = pred_result.cpu()\n",
        "      #acc_token = acc_token(cur_decode_out, pred_result)\n",
        "\n",
        "      is_correct = check_correct(pred_result, cur_decode_out)\n",
        "      total_correct_num += is_correct\n",
        "      total_img_num += 1\n",
        "      # if not is_correct:\n",
        "      #     print(\"----\")\n",
        "      #     print(\"имеющаяся последовательность\", cur_decode_out)\n",
        "      #     print(\"предсказанная последовательность\", pred_result)\n",
        "\n",
        "end = time.time()\n",
        "total_correct_rate = total_correct_num / total_img_num * 100\n",
        "print(f\"total correct rate of trainset: {total_correct_rate}%\")\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end-start) * 10**3, \"ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRZun91XW7p1"
      },
      "outputs": [],
      "source": [
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"greedy decode validset\")\n",
        "total_img_num = 0\n",
        "total_correct_num = 0\n",
        "start = time.time()\n",
        "for batch_idx, batch in enumerate(valid_loader):\n",
        "  img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "  img_input = img_input.to(device)                        \n",
        "  encode_mask = encode_mask.to(device)                                \n",
        "\n",
        "  bs = img_input.shape[0]\n",
        "  for i in range(bs):\n",
        "      cur_img_input = img_input[i].unsqueeze(0)\n",
        "      cur_encode_mask = encode_mask[i].unsqueeze(0)\n",
        "      cur_decode_out = decode_out[i]\n",
        "      \n",
        "      pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=11, start_symbol=1, end_symbol=2)\n",
        "      pred_result = pred_result.cpu()\n",
        "\n",
        "      is_correct = check_correct(pred_result, cur_decode_out)\n",
        "      total_correct_num += is_correct\n",
        "      total_img_num += 1\n",
        "      # if not is_correct:\n",
        "      #     print(\"имеющаяся последовательность\", cur_decode_out)\n",
        "      #     print(\"предсказанная последовательность\", pred_result)\n",
        "\n",
        "end = time.time()\n",
        "total_correct_rate = total_correct_num / total_img_num * 100\n",
        "print(f\"total correct rate of validset: {total_correct_rate}%\")\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end-start) * 10**3, \"ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB9B0RrckTYG"
      },
      "outputs": [],
      "source": [
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"greedy decode testset\")\n",
        "total_img_num = 0\n",
        "total_correct_num = 0\n",
        "start = time.time()\n",
        "for batch_idx, batch in enumerate(test_loader):\n",
        "  img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch\n",
        "  img_input = img_input.to(device)                        \n",
        "  encode_mask = encode_mask.to(device)                                \n",
        "\n",
        "  bs = img_input.shape[0]\n",
        "  for i in range(bs):\n",
        "      cur_img_input = img_input[i].unsqueeze(0)\n",
        "      cur_encode_mask = encode_mask[i].unsqueeze(0)\n",
        "      cur_decode_out = decode_out[i]\n",
        "      \n",
        "      pred_result = greedy_decode(ocr_model, cur_img_input, cur_encode_mask, max_len=11, start_symbol=1, end_symbol=2)\n",
        "      pred_result = pred_result.cpu()\n",
        "\n",
        "      is_correct = check_correct(pred_result, cur_decode_out)\n",
        "      total_correct_num += is_correct\n",
        "      total_img_num += 1\n",
        "      # if not is_correct:\n",
        "      #     print(\"имеющаяся последовательность\", cur_decode_out)\n",
        "      #     print(\"предсказанная последовательность\", pred_result)\n",
        "end = time.time()\n",
        "total_correct_rate = total_correct_num / total_img_num * 100\n",
        "print(f\"total correct rate of testset: {total_correct_rate}%\")\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end-start) * 10**3, \"ms\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1E4Qj5dq_1R1av-M9PFLfECDVYKEHh0JV",
      "authorship_tag": "ABX9TyMFtp5J8SdmyZ17IfNNPRwU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5990f2e1df242c0b685503bb8520168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d683c3d3ac142f8b2df6334324b7c18",
              "IPY_MODEL_b1caeebf1803444f9c92684ff3b6b3ef",
              "IPY_MODEL_065cdb2accc14865a98f2e20aee96286"
            ],
            "layout": "IPY_MODEL_9fe15609180f46b3a9ebeacce7551583"
          }
        },
        "7d683c3d3ac142f8b2df6334324b7c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c5a054dff54fe08b67ddc8c0c8e376",
            "placeholder": "​",
            "style": "IPY_MODEL_57138c11cef545a2adc50fd6cef16bdf",
            "value": "100%"
          }
        },
        "b1caeebf1803444f9c92684ff3b6b3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c1a02124231401bacaaef76dd2edb2a",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41800efcb7b8474885f9968657c9fcb7",
            "value": 46830571
          }
        },
        "065cdb2accc14865a98f2e20aee96286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_542e804d63c2465391aa86b4c4fb62d0",
            "placeholder": "​",
            "style": "IPY_MODEL_2793b786813047d99d49fe5efacf1bd1",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 159MB/s]"
          }
        },
        "9fe15609180f46b3a9ebeacce7551583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74c5a054dff54fe08b67ddc8c0c8e376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57138c11cef545a2adc50fd6cef16bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c1a02124231401bacaaef76dd2edb2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41800efcb7b8474885f9968657c9fcb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "542e804d63c2465391aa86b4c4fb62d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2793b786813047d99d49fe5efacf1bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}